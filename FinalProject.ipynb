{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0db5780818ef411abb02f25b51fa7a13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_859765efe33b4fe8a5de94fcb958c29f","IPY_MODEL_562beb6fcaee41898c1376cc49f2a731","IPY_MODEL_32858879253d44a88af6740ac2d8709f"],"layout":"IPY_MODEL_ad29a2bde1b14c0eb3e83caa3480cc9c"}},"859765efe33b4fe8a5de94fcb958c29f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_198822e942f349f7b44b60f2272b0886","placeholder":"​","style":"IPY_MODEL_893ce291e9584876878ffbd669f1d067","value":"Computing transition probabilities: 100%"}},"562beb6fcaee41898c1376cc49f2a731":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46ab7eb538d742ffb00ab399be1abdca","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0be23950e451453db9880a729c3621c3","value":63}},"32858879253d44a88af6740ac2d8709f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92a4a069f0be4e28a2250cad5985ec66","placeholder":"​","style":"IPY_MODEL_721409a85bd74008ae38e66bb522f33e","value":" 63/63 [00:00&lt;00:00, 4045.27it/s]"}},"ad29a2bde1b14c0eb3e83caa3480cc9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198822e942f349f7b44b60f2272b0886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"893ce291e9584876878ffbd669f1d067":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46ab7eb538d742ffb00ab399be1abdca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0be23950e451453db9880a729c3621c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92a4a069f0be4e28a2250cad5985ec66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"721409a85bd74008ae38e66bb522f33e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"NFcP2YkliYea"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fqu9QrpiiQa9","executionInfo":{"status":"ok","timestamp":1734118412672,"user_tz":300,"elapsed":14892,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"dbd3f2a2-2d23-4e79-8866-c82488833042"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","# Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a EECS595 folder and put all the files under HW1 folder, then 'EECS595/HW1'\n","\n","# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #\n","#           TODO: Change this to your project path            #\n","# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"CSE 595/FinalProject\"\n","\n","# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #"]},{"cell_type":"code","source":["import sys\n","import os\n","GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","# www_path = os.path.join(GOOGLE_DRIVE_PATH, 'www')\n","# sys.path.append(www_path)\n","# dataset_path = os.path.join(GOOGLE_DRIVE_PATH, 'www/dataset')\n","# sys.path.append(dataset_path)\n","# model_path = os.path.join(GOOGLE_DRIVE_PATH, 'www/model')\n","# sys.path.append(model_path)\n","\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","print(os.listdir(os.path.join(GOOGLE_DRIVE_PATH, \"www\")))\n","# print(os.listdir(www_path))\n","# print(os.listdir(dataset_path))\n","# print(os.listdir(model_path))\n","\n","# print(GOOGLE_DRIVE_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbPqn78lq_dt","executionInfo":{"status":"ok","timestamp":1734118414014,"user_tz":300,"elapsed":1345,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"8683f5e6-557d-4a73-b3e6-94daedce7fc9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['data', 'cache', 'www', 'TODO.gdoc', 'saved_models', 'BertFeaturescopy.ipynb', 'baseline.ipynb', 'spacey.ipynb', 'CSE 595 Final Presentation.gslides', 'FinalProject.ipynb']\n","['utils.py', 'dataset', 'model', '__pycache__']\n"]}]},{"cell_type":"code","source":["print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xaaY5g5k4la","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":5,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"b52cf6b0-faa9-4483-fb29-0c94c0dffe86"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', 'drive/My Drive/CSE 595/FinalProject']\n"]}]},{"cell_type":"code","source":["mode = 'bert' # BERT large"],"metadata":{"id":"70pHDwsokHK9","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":4,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["task_name = 'ce'"],"metadata":{"id":"FkfM1mvnkKl1","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":4,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["debug = False"],"metadata":{"id":"y3lTyuhNuA-z","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":4,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"TB1Zwhs2kdix"}},{"cell_type":"code","source":["config_batch_size = 32\n","config_lr = 7.5e-6 # Selected learning rate for best BERT-based model in TRIP paper\n","config_epochs = 8"],"metadata":{"id":"LtRxhzeZkM5N","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":4,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["For evaluating models"],"metadata":{"id":"vGwIBXbBkbD3"}},{"cell_type":"code","source":["eval_model_dir = 'bert-large-uncased_ConvEnt_32_7.5e-06_7_xval'\n","# eval_model_dir = 'bert-large-uncased_ConvEnt_1_1e-05_0_xval'\n","# eval_model_dir = 'bert-large-uncased_ConvEnt_32_1e-05_7_xval'"],"metadata":{"id":"ttWkMEExkQBP","executionInfo":{"status":"ok","timestamp":1734118787124,"user_tz":300,"elapsed":110,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["##Colab Setup"],"metadata":{"id":"A_fuOdpQkiky"}},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"sOAZhZqRkg0v","executionInfo":{"status":"ok","timestamp":1734118414015,"user_tz":300,"elapsed":3,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##Model Setup"],"metadata":{"id":"cc69-iPsktCR"}},{"cell_type":"code","source":["!pip install coreferee\n","!pip install spacy\n","!pip install spacy-transformers\n","!python -m coreferee install en\n","!python3 -m spacy download en_core_web_trf\n","!python3 -m spacy download en_core_web_lg\n","!python3 -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uArSogvV8po2","executionInfo":{"status":"ok","timestamp":1734118524101,"user_tz":300,"elapsed":110089,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"608028c5-d6f8-40cc-87fa-ad1ff96f6131"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting coreferee\n","  Downloading coreferee-1.4.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting spacy<3.6.0,>=3.0.0 (from coreferee)\n","  Downloading spacy-3.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (3.0.9)\n","Collecting thinc<8.2.0,>=8.1.8 (from spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (2.0.10)\n","Collecting typer<0.10.0,>=0.3.0 (from spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n","Collecting pathy>=0.10.0 (from spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n","Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (4.66.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (2.32.3)\n","Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.0.0->coreferee) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.0.0->coreferee) (1.3.0)\n","Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy<3.6.0,>=3.0.0->coreferee)\n","  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.0.0->coreferee) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.0.0->coreferee) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.0.0->coreferee) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.0.0->coreferee) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.0.0->coreferee) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.0.0->coreferee) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.0.0->coreferee) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.0.0->coreferee) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.0.0->coreferee) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.0.0->coreferee) (1.2.1)\n","Downloading coreferee-1.4.1-py3-none-any.whl (182 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.6/182.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spacy-3.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n","Downloading pydantic-1.10.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: typer, smart-open, pydantic, pathlib-abc, pathy, thinc, spacy, coreferee\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.15.1\n","    Uninstalling typer-0.15.1:\n","      Successfully uninstalled typer-0.15.1\n","  Attempting uninstall: smart-open\n","    Found existing installation: smart-open 7.0.5\n","    Uninstalling smart-open-7.0.5:\n","      Successfully uninstalled smart-open-7.0.5\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.10.3\n","    Uninstalling pydantic-2.10.3:\n","      Successfully uninstalled pydantic-2.10.3\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.2.5\n","    Uninstalling thinc-8.2.5:\n","      Successfully uninstalled thinc-8.2.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.7.5\n","    Uninstalling spacy-3.7.5:\n","      Successfully uninstalled spacy-3.7.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n","en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.5.4 which is incompatible.\n","google-genai 0.1.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.19 which is incompatible.\n","langchain 0.3.11 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.19 which is incompatible.\n","langchain-core 0.3.24 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed coreferee-1.4.1 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.19 smart-open-6.4.0 spacy-3.5.4 thinc-8.1.12 typer-0.9.4\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.19)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Collecting spacy-transformers\n","  Downloading spacy_transformers-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (3.5.4)\n","Collecting transformers<4.37.0,>=3.4.0 (from spacy-transformers)\n","  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.5.1+cu121)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.5.0)\n","Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n","  Downloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (1.26.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.9.4)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.10.19)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.26.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (2024.9.11)\n","Collecting tokenizers<0.19,>=0.14 (from transformers<4.37.0,>=3.4.0->spacy-transformers)\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.4.5)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n","Downloading spacy_transformers-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: spacy-alignments, tokenizers, transformers, spacy-transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.3\n","    Uninstalling tokenizers-0.20.3:\n","      Successfully uninstalled tokenizers-0.20.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.3\n","    Uninstalling transformers-4.46.3:\n","      Successfully uninstalled transformers-4.46.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed spacy-alignments-0.9.1 spacy-transformers-1.3.5 tokenizers-0.15.2 transformers-4.36.2\n","2024-12-13 19:33:59.830753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-13 19:33:59.851641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-13 19:33:59.858019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Collecting https://github.com/richardpaulhudson/coreferee/raw/master/models/coreferee_model_en.zip\n","  Downloading https://github.com/richardpaulhudson/coreferee/raw/master/models/coreferee_model_en.zip (65.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: coreferee-model-en\n","  Building wheel for coreferee-model-en (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for coreferee-model-en: filename=coreferee_model_en-1.0.0-py3-none-any.whl size=65422507 sha256=1ea576cbd3a0d7f8c8bce167306affe15bf8ffbe8749806e3b5f88e1436593fc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m6sdd0r1/wheels/87/ca/93/b7c91eabb7b65a4200a75ea842a7009810982015d2dedfdc54\n","Successfully built coreferee-model-en\n","Installing collected packages: coreferee-model-en\n","Successfully installed coreferee-model-en-1.0.0\n","2024-12-13 19:34:21.871465: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-12-13 19:34:21.888773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-13 19:34:21.909750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-13 19:34:21.916359: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-13 19:34:21.931314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-13 19:34:23.089120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-trf==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.5.0/en_core_web_trf-3.5.0-py3-none-any.whl (460.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.3/460.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.5.0) (3.5.4)\n","Collecting spacy-transformers<1.3.0,>=1.2.0.dev0 (from en-core-web-trf==3.5.0)\n","  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.9.4)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.66.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.10.19)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.5.0)\n","Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0)\n","  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2.5.1+cu121)\n","Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.9.1)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.1.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.4.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.26.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2024.9.11)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.4.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.2.1)\n","Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers, spacy-transformers, en-core-web-trf\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.36.2\n","    Uninstalling transformers-4.36.2:\n","      Successfully uninstalled transformers-4.36.2\n","  Attempting uninstall: spacy-transformers\n","    Found existing installation: spacy-transformers 1.3.5\n","    Uninstalling spacy-transformers-1.3.5:\n","      Successfully uninstalled spacy-transformers-1.3.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed en-core-web-trf-3.5.0 spacy-transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_trf')\n","2024-12-13 19:34:50.793027: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-12-13 19:34:50.810110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-13 19:34:50.830934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-13 19:34:50.837284: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-13 19:34:50.852144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-13 19:34:52.030362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-lg==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.9.4)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.66.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.19)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.2.1)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n","2024-12-13 19:35:16.723703: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-12-13 19:35:16.741036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-13 19:35:16.761615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-13 19:35:16.767887: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-13 19:35:16.782683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-13 19:35:17.931840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.4)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.66.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.19)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.2.1)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.7.1\n","    Uninstalling en-core-web-sm-3.7.1:\n","      Successfully uninstalled en-core-web-sm-3.7.1\n","Successfully installed en-core-web-sm-3.5.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["!pip install jsonlines\n","!pip install transformers\n","!pip install sentencepiece\n","!pip3 install torch torchvision torchaudio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLfEtS2JkvXh","executionInfo":{"status":"ok","timestamp":1734118534425,"user_tz":300,"elapsed":10327,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"3557028c-e2cc-4697-d95a-8a9f599bf1b7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jsonlines\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n","Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-4.0.0\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","import random\n","import numpy as np\n","import spacy_transformers\n","import coreferee\n","import spacy"],"metadata":{"id":"DPFCZEiAkpRT","executionInfo":{"status":"ok","timestamp":1734118542395,"user_tz":300,"elapsed":7973,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Model Components"],"metadata":{"id":"oidUPLb4lAft"}},{"cell_type":"code","source":["if task_name in ['trip', 'ce']:\n","  multiple_choice = False\n","elif task_name == 'art':\n","  multiple_choice = True\n","else:\n","  raise ValueError(\"Task name should be set to 'trip', 'ce', or 'art' in the first cell of the notebook!\")\n","\n","if mode == 'bert':\n","  model_name = 'bert-large-uncased'"],"metadata":{"id":"1RkH8_3ClCND","executionInfo":{"status":"ok","timestamp":1734118768490,"user_tz":300,"elapsed":134,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["Model tokenizer"],"metadata":{"id":"_7JnzGmalFGV"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","# from DeBERTa import deberta\n","if mode in ['bert']:\n","  tokenizer_class = BertTokenizer\n","\n","tokenizer = tokenizer_class.from_pretrained(model_name,\n","                                                do_lower_case = False,\n","                                                cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFSCWeHtlFT3","executionInfo":{"status":"ok","timestamp":1734118770014,"user_tz":300,"elapsed":309,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"23716d70-930d-41be-cd4b-97963edbbe33"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Load model and optimizer"],"metadata":{"id":"5WQctXpMlYuf"}},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AlbertForSequenceClassification, AdamW\n","from transformers import BertModel, AlbertModel, T5Model, T5EncoderModel, GPT2Model\n","from transformers import BertConfig, AlbertConfig, T5Config, GPT2Config\n","from torch.optim import Adam\n","\n","if mode == 'bert':\n","  model_class = BertForSequenceClassification\n","  config_class = BertConfig\n","  emb_class = BertModel"],"metadata":{"id":"kmVn-QnxlY3g","executionInfo":{"status":"ok","timestamp":1734118778824,"user_tz":300,"elapsed":93,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["#Baseline BERT"],"metadata":{"id":"6xZOXw-nnQ-T"}},{"cell_type":"markdown","source":["##Load Conversational Entailment Data"],"metadata":{"id":"F-wE_htuoPQh"}},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\n","import pickle\n","cache_train = os.path.join(GOOGLE_DRIVE_PATH, 'data/ConvEnt_train_resplit.json')\n","cache_dev = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_resplit.json')\n","cache_test = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_test_resplit.json')\n","ConvEnt_train = json.load(open(cache_train))\n","ConvEnt_dev = json.load(open(cache_dev))\n","ConvEnt_test = json.load(open(cache_test))\n","\n","# Combine train and dev and do cross-validation\n","cache_folds = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_folds.pkl') # Folds used for results presented in paper\n","ConvEnt_train = ConvEnt_train + ConvEnt_dev\n","train_sources = list(set([ex['dialog_source'] for ex in ConvEnt_train]))\n","print(\"Reserved %s dialog sources for training and validation.\" % len(train_sources))\n","\n","no_folds = 8\n","if not os.path.exists(cache_folds):\n","  folds = []\n","  for k in range(no_folds):\n","    folds.append(np.random.choice(train_sources, size=5, replace=False))\n","    train_sources = [s for s in train_sources if s not in folds[-1]]\n","  assert len(train_sources) == 0\n","  print(folds)\n","  pickle.dump(folds, open(cache_folds, 'wb'))\n","else:\n","  folds = pickle.load(open(cache_folds, 'rb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GgFwwVgnYRB","executionInfo":{"status":"ok","timestamp":1734118795378,"user_tz":300,"elapsed":168,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"ca6b8fc8-5d96-4aa2-f04e-67b80ab976a4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Reserved 40 dialog sources for training and validation.\n"]}]},{"cell_type":"code","source":["print('train examples:', len(ConvEnt_train))\n","print('dev examples:', len(ConvEnt_dev))\n","print('test examples:', len(ConvEnt_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mshtLzUHpIuX","executionInfo":{"status":"ok","timestamp":1734118796880,"user_tz":300,"elapsed":88,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"fd9a5028-f37b-40d8-9c42-14e257c3d0a4"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["train examples: 703\n","dev examples: 110\n","test examples: 172\n"]}]},{"cell_type":"markdown","source":["##Featurize Data"],"metadata":{"id":"titVzxY9trX_"}},{"cell_type":"code","source":["from www.dataset.featurize import add_bert_features_ConvEnt, get_tensor_dataset\n","import pickle\n","seq_length = 128\n","\n","ConvEnt_train = add_bert_features_ConvEnt(ConvEnt_train, tokenizer, seq_length, add_segment_ids=True)\n","ConvEnt_dev = add_bert_features_ConvEnt(ConvEnt_dev, tokenizer, seq_length, add_segment_ids=True)\n","ConvEnt_test = add_bert_features_ConvEnt(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True)\n","\n","ConvEnt_train_folds = [[] for _ in range(no_folds)]\n","ConvEnt_dev_folds = [[] for _ in range(no_folds)]\n","for k in range(no_folds):\n","  ConvEnt_train_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] not in folds[k]]\n","  ConvEnt_dev_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] in folds[k]]\n","\n","  if debug:\n","    ConvEnt_train_folds[k] = ConvEnt_train_folds[k][:10]\n","    ConvEnt_dev_folds[k] = ConvEnt_dev_folds[k][:10]\n","\n","if debug:\n","  ConvEnt_train = ConvEnt_train[:10]\n","  ConvEnt_dev = ConvEnt_dev[:10]\n","  ConvEnt_test = ConvEnt_test[:10]\n","\n","ConvEnt_train_tensor = get_tensor_dataset(ConvEnt_train, label_key='label', add_segment_ids=True)\n","ConvEnt_test_tensor = get_tensor_dataset(ConvEnt_test, label_key='label', add_segment_ids=True)\n","\n","# Training sets for each validation fold\n","ConvEnt_train_folds_tensor = [get_tensor_dataset(ConvEnt_train_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]\n","ConvEnt_dev_folds_tensor = [get_tensor_dataset(ConvEnt_dev_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgGcO2Ygtrhs","executionInfo":{"status":"ok","timestamp":1734118799967,"user_tz":300,"elapsed":1818,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"f93441bc-ddcf-4b4f-902d-a2665d28d789"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]}]},{"cell_type":"code","source":["print('train examples:', len(ConvEnt_train))\n","print('dev examples:', len(ConvEnt_dev))\n","print('test examples:', len(ConvEnt_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoO8pkKCt5yZ","executionInfo":{"status":"ok","timestamp":1734118803984,"user_tz":300,"elapsed":778,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"00363ecf-0759-4768-b6bd-f1446ed4bc37"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["train examples: 703\n","dev examples: 110\n","test examples: 172\n"]}]},{"cell_type":"markdown","source":["##Train Model"],"metadata":{"id":"UolEWaq1l_XT"}},{"cell_type":"code","source":["batch_sizes = [config_batch_size]\n","learning_rates = [config_lr]\n","epochs = config_epochs\n","eval_batch_size = 128"],"metadata":{"id":"LOWoQLcJmBr6","executionInfo":{"status":"ok","timestamp":1734118804324,"user_tz":300,"elapsed":104,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.train import train_epoch\n","from www.model.eval import evaluate, save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","from collections import Counter\n","\n","seed_val = 22 # Save random seed for reproducibility\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","assert len(batch_sizes) == 1\n","train_fold_sampler = [RandomSampler(f) for f in ConvEnt_train_folds_tensor]\n","train_fold_dataloader = [DataLoader(f, sampler=train_fold_sampler[i], batch_size=batch_sizes[0]) for i, f in enumerate(ConvEnt_train_folds_tensor)]\n","\n","dev_fold_sampler = [SequentialSampler(f) for f in ConvEnt_dev_folds_tensor]\n","dev_fold_dataloader = [DataLoader(f, sampler=dev_fold_sampler[i], batch_size=eval_batch_size) for i, f in enumerate(ConvEnt_dev_folds_tensor)]\n","\n","all_val_accs = Counter()\n","print('Beginning grid search for ConvEnt over %s parameter combination(s)!' % (str(len(batch_sizes) * len(learning_rates))))\n","for bs in batch_sizes:\n","  for lr in learning_rates:\n","    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n","\n","    for k in range(no_folds):\n","      print('Beginning fold %s/%s...' % (str(k+1), str(no_folds)))\n","\n","      # Set up model\n","      if 'mnli' not in mode:\n","        model = model_class.from_pretrained(model_name,\n","                                            cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","      else:\n","        config = config_class.from_pretrained(model_name.replace('-mnli',''),\n","                                        num_labels=3,\n","                                        cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","        model = model_class.from_pretrained(model_name,\n","                                            config=config,\n","                                            cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","        config.num_labels = 2\n","        model.num_labels = 2\n","        model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n","\n","      model.cuda()\n","      device = model.device\n","\n","      # Set up optimizer\n","      optimizer = AdamW(model.parameters(), lr=lr)\n","      total_steps = len(train_fold_dataloader[k]) * epochs\n","      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n","\n","      for epoch in range(epochs):\n","        # Train the model for one epoch\n","        print('[%s] Beginning epoch...' % str(epoch))\n","\n","        epoch_loss, _ = train_epoch(model, optimizer, train_fold_dataloader[k], device, seg_mode=True if 'roberta' not in mode else False)\n","\n","        # Validate on dev set\n","        results, _, _ = evaluate(model, dev_fold_dataloader[k], device, [(accuracy_score, 'accuracy')], seg_mode=True if 'roberta' not in mode else False)\n","        print('[%s] Validation results:' % str(epoch))\n","        print_dict(results)\n","\n","        # Save accuracy\n","        acc = results['accuracy']\n","        if (bs, lr, epoch) in all_val_accs:\n","          all_val_accs[(bs, lr, epoch)] += acc\n","        else:\n","          all_val_accs[(bs, lr, epoch)] = acc\n","\n","      model.cpu()\n","      del model\n","      del optimizer\n","      del results\n","      del scheduler\n","      del total_steps\n","\n","      print('[%s] Finished epoch.' % str(epoch))\n","\n","for k in all_val_accs:\n","  all_val_accs[k] /= no_folds\n","\n","print('Top performing param combos:')\n","print(all_val_accs.most_common(5))\n","\n","save_fname = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models/%s_ConvEnt_xval_%s.pkl' % (model_name.replace('/','-'), '_'.join([str(lr) for lr in learning_rates])))\n","pickle.dump(all_val_accs, open(save_fname, 'wb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdfxIq91mCam","executionInfo":{"status":"ok","timestamp":1734113829755,"user_tz":300,"elapsed":585739,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"d42f17c0-0b76-4d2b-b8c5-3a0b8bad05e6"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Beginning grid search for ConvEnt over 1 parameter combination(s)!\n","\n","TRAINING MODEL: bs=32, lr=7.5e-06\n","Beginning fold 1/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5277777777777778,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5833333333333334,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5555555555555556,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5740740740740741,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5277777777777778,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5648148148148148,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 2/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5581395348837209,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5465116279069767,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5930232558139535,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 3/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.3815789473684211,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5394736842105263,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5394736842105263,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.6447368421052632,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5789473684210527,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5526315789473685,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5657894736842105,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.618421052631579,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 4/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.4942528735632184,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.42528735632183906,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.4942528735632184,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5057471264367817,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5172413793103449,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5287356321839081,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.41379310344827586,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.4827586206896552,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 5/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5625,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5375,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5875,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 6/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.4935064935064935,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5454545454545454,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5974025974025974,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.6233766233766234,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.6623376623376623,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.6363636363636364,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 7/8...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5274725274725275,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5384615384615384,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5274725274725275,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.4725274725274725,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5274725274725275,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.4945054945054945,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 8/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5204081632653061,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5510204081632653,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5510204081632653,\n","}\n","\n","\n","[3] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5714285714285714,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5204081632653061,\n","}\n","\n","\n","[5] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5816326530612245,\n","}\n","\n","\n","[6] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5714285714285714,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:02s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5816326530612245,\n","}\n","\n","\n","[7] Finished epoch.\n","Top performing param combos:\n","[((32, 7.5e-06, 7), 0.5698774409850447), ((32, 7.5e-06, 5), 0.556702196131799), ((32, 7.5e-06, 3), 0.5517702446525041), ((32, 7.5e-06, 6), 0.5440763430070003), ((32, 7.5e-06, 4), 0.540253448928423)]\n"]}]},{"cell_type":"markdown","source":["##Re-Train Best Model from Cross-Validation"],"metadata":{"id":"fSNpULCpm5k6"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.train import train_epoch\n","from www.model.eval import evaluate, save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","from collections import Counter\n","\n","# Re-train the model with the best parameters from the grid search/cross-validation (with all folds)\n","xval_fnames = []\n","xval_fnames.append(save_fname.split('/')[-1])\n","\n","xval_results = Counter()\n","for fname in xval_fnames:\n","  xval_results += pickle.load(open(os.path.join(GOOGLE_DRIVE_PATH, 'saved_models/', fname), 'rb'))\n","\n","batch_size, learning_rate, epochs = xval_results.most_common(1)[0][0]\n","epochs += 1\n","\n","# Set up model\n","if 'mnli' not in mode:\n","  model = model_class.from_pretrained(model_name,\n","                                      cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","else:\n","  config = config_class.from_pretrained(model_name.replace('-mnli',''),\n","                                  num_labels=3,\n","                                  cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","  model = model_class.from_pretrained(model_name,\n","                                      config=config,\n","                                      cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","  config.num_labels = 2\n","  model.num_labels = 2\n","  model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n","\n","model.cuda()\n","device = model.device\n","\n","train_sampler = RandomSampler(ConvEnt_train_tensor)\n","train_dataloader = DataLoader(ConvEnt_train_tensor, sampler=train_sampler, batch_size=batch_size)\n","\n","# Set up optimizer\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n","\n","for epoch in range(epochs):\n","  print('[%s] Beginning epoch...' % str(epoch))\n","  epoch_loss, _ = train_epoch(model, optimizer, train_dataloader, device, seg_mode=True if 'roberta' not in mode else False)\n","\n","print('[%s] Saving model checkpoint...' % str(epoch))\n","model_param_str = get_model_dir(model_name.replace('/','-'), 'ConvEnt', batch_size, learning_rate, epoch) + '_xval'\n","output_dir = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', model_param_str)\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","model = model.module if hasattr(model, 'module') else model\n","model.save_pretrained(output_dir)\n","tokenizer.save_vocabulary(output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLLFtOM9nHsG","executionInfo":{"status":"ok","timestamp":1734118702225,"user_tz":300,"elapsed":126471,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"94051894-b5e0-4f10-c51b-cdce300137a9"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","[1] Beginning epoch...\n","[2] Beginning epoch...\n","[3] Beginning epoch...\n","[4] Beginning epoch...\n","[5] Beginning epoch...\n","[6] Beginning epoch...\n","[7] Beginning epoch...\n","[7] Saving model checkpoint...\n"]},{"output_type":"execute_result","data":{"text/plain":["('drive/My Drive/CSE 595/FinalProject/saved_models/bert-large-uncased_ConvEnt_32_1e-05_7_xval/vocab.txt',)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["##Test Models on Conversational Entailment"],"metadata":{"id":"CLvrtxcPn0kG"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.eval import evaluate, save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","\n","best_model = eval_model_dir\n","\n","\n","best_model = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', best_model)\n","# print(best_model)\n","# Load the model\n","model = model_class.from_pretrained(best_model)\n","model.cuda()\n","device = model.device\n","\n","# Select appropriate dataset\n","if 'cloze' in best_model:\n","  subtask = 'cloze'\n","elif 'order' in best_model:\n","  subtask = 'order'\n","\n","test_sampler = SequentialSampler(ConvEnt_test_tensor)\n","test_dataloader = DataLoader(ConvEnt_test_tensor, sampler=test_sampler, batch_size=128)\n","test_dataset_name = '%s_%s' % ('ConvEnt', 'test')\n","test_ids = [str(ex['example_id']) for ex in ConvEnt_test]\n","\n","print('Testing model: %s.' % best_model.split('/')[-1])\n","\n","results, preds, labels = evaluate(model, test_dataloader, device, [(accuracy_score, 'accuracy')], seg_mode=True if 'roberta' not in mode else False)\n","save_results(results, best_model, test_dataset_name)\n","save_preds(test_ids, labels, preds, best_model, test_dataset_name)\n","\n","print('Results:')\n","print_dict(results)"],"metadata":{"id":"sVy-XW0kn1ST","executionInfo":{"status":"ok","timestamp":1734118851977,"user_tz":300,"elapsed":18536,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4f41117-8fa5-4413-bafc-f03595c20c9d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=\"cpu\")\n"]},{"output_type":"stream","name":"stdout","text":["Testing model: bert-large-uncased_ConvEnt_32_7.5e-06_7_xval.\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:01s.\n","Results:\n","{\n","  accuracy: \n","    0.5755813953488372,\n","}\n","\n","\n"]}]},{"cell_type":"markdown","source":["##Coherence Checks on Conversational Entailment"],"metadata":{"id":"YD6SAJiWudFg"}},{"cell_type":"markdown","source":["###Load and Featurize Span Data"],"metadata":{"id":"H4AU0jqrupmB"}},{"cell_type":"code","source":["from www.dataset.featurize import add_bert_features_ConvEnt, get_tensor_dataset\n","from www.dataset.prepro import get_ConvEnt_spans\n","import pickle\n","seq_length = 128\n","\n","merged_file = os.path.join(GOOGLE_DRIVE_PATH, 'data/ConvEnt_test_annotation_merged2.json')\n","ConvEnt_test = json.load(open(merged_file))\n","\n","ConvEnt_test = add_bert_features_ConvEnt(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True)\n","\n","if debug:\n","  ConvEnt_test = ConvEnt_test[:10]\n","\n","# Some of the annotated examples are no longer in the test set :(\n","# ConvEnt_test = [ex for ex in ConvEnt_test if ex['id'] in test_ids]\n","\n","# Make span versions of the datasets\n","ConvEnt_test_spans = get_ConvEnt_spans(ConvEnt_test)\n","\n","# Add BERT features\n","ConvEnt_test_tensor = get_tensor_dataset(ConvEnt_test, label_key='label', add_segment_ids=True)\n","ConvEnt_test_spans_tensor = get_tensor_dataset(ConvEnt_test_spans, label_key='label', add_segment_ids=True)"],"metadata":{"id":"D31p6hdAuyA7","executionInfo":{"status":"ok","timestamp":1734118852270,"user_tz":300,"elapsed":297,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2b43e73-9c16-4952-f356-5047714b795b"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]}]},{"cell_type":"markdown","source":["###Load the Trained Model"],"metadata":{"id":"F97v7fLKuuOx"}},{"cell_type":"code","source":["probe_model = eval_model_dir\n","probe_model = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', probe_model)\n","\n","# Load the model\n","model = model_class.from_pretrained(probe_model)\n","if torch.cuda.is_available():\n","  model.cuda()\n","device = model.device"],"metadata":{"id":"95yH2wz2uyaw","executionInfo":{"status":"ok","timestamp":1734118856964,"user_tz":300,"elapsed":4695,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"374dded8-dd9b-4214-b484-d996f2c0d4b8"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=\"cpu\")\n"]}]},{"cell_type":"code","source":["from www.model.eval import load_preds\n","from www.utils import print_dict\n","\n","preds_base = {}\n","preds_base['test'] = load_preds(os.path.join(probe_model, 'preds_ConvEnt_test.tsv'))\n","print(preds_base['test'].keys())"],"metadata":{"id":"0lDfSNhqvcgJ","executionInfo":{"status":"ok","timestamp":1734118856964,"user_tz":300,"elapsed":2,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"32500339-8080-411c-b61b-af1a88e560cd"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '272', '273', '274', '275', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838'])\n"]}]},{"cell_type":"markdown","source":["###Check a Model"],"metadata":{"id":"2xiuDaGpvgPa"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from www.model.eval import evaluate, save_results, save_preds, list_comparison\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n","import numpy as np\n","from www.utils import print_dict\n","\n","def is_polarized(smax, thres):\n","  return (abs(smax[0] - smax[1]) >= thres)\n","\n","print('Testing model: %s.' % probe_model)\n","\n","all_results = {}\n","p = 'test'\n","\n","p_dataset = ConvEnt_test_spans\n","p_tensor_dataset = ConvEnt_test_spans_tensor\n","p_sampler = SequentialSampler(p_tensor_dataset)\n","p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=512)\n","p_dataset_name = '%s_spans_%s' % ('ConvEnt', p)\n","p_dataset_name_co = '%s_consistent_%s' % ('ConvEnt', p)\n","p_dataset_name_bp = '%s_breakpoints_%s' % ('ConvEnt', p)\n","p_dataset_name_ev = '%s_evidence_%s' % ('ConvEnt', p)\n","p_dataset_name_coh = '%s_coherent_%s' % ('ConvEnt', p)\n","p_ids = [str(ex['example_id']) for ex in ConvEnt_test_spans]\n","p_labels = [ex['label'] for ex in ConvEnt_test_spans]\n","\n","# Get span preds and save metrics\n","results, preds, labels = evaluate(model, p_dataloader, device, metrics, seg_mode=True)\n","save_results(results, probe_model, p_dataset_name)\n","save_preds(p_ids, labels, preds, probe_model, p_dataset_name)\n","\n","# Convert substory preds into breakpoint preds for each example\n","ids_base = [str(ex['example_id']) for ex in ConvEnt_test]\n","\n","id_to_pred = {k: v for k,v in zip(p_ids, preds)}\n","id_to_label = {k: v for k,v in zip(p_ids, p_labels)}\n","\n","preds_entailment = []\n","labels_entailment = []\n","preds_consistent = []\n","preds_breakpoint = []\n","labels_breakpoint = []\n","preds_evidence = []\n","labels_evidence = []\n","span_accuracies = []\n","span_accuracies_strict = []\n","preds_coherent = []\n","\n","for i, exid in enumerate(ids_base):\n","  ex = ConvEnt_test[i]\n","  ex['length'] = len(ex['turns'])\n","\n","  label_entailment = preds_base[p][exid]['label']\n","  pred_entailment = preds_base[p][exid]['pred']\n","  labels_entailment.append(label_entailment)\n","  preds_entailment.append(pred_entailment)\n","\n","  # Get ground truth breakpoint and evidence\n","  label_breakpoint = ex['conflict_pair'][1] if ex['conflict_pair'] is not None and len(ex['conflict_pair']) > 0 else 0\n","  labels_breakpoint.append(label_breakpoint)\n","  if label_breakpoint > 0:\n","    label_ev = ex['conflict_pair'][0]\n","  else:\n","    label_ev = -1\n","  labels_evidence.append(label_ev)\n","\n","  # Check consistency - any span that entails the hypothesis' superspans should also entail\n","  pred_consistent = True\n","  span_accuracy = 0.0\n","  span_accuracy_strict = 0.0\n","  pred_coherent = True\n","\n","  no_spans = 0\n","  for sp1 in range(ex['length']):\n","    if not pred_consistent:\n","      break\n","\n","    for sp2 in range(sp1, ex['length']):\n","      if not pred_consistent:\n","        break\n","\n","      span_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n","      span_label = id_to_label[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n","\n","      if span_pred == span_label:\n","        span_accuracy += 1.0\n","        if label_entailment == pred_entailment:\n","            span_accuracy_strict += 1.0\n","      else:\n","        pred_coherent = False\n","      no_spans += 1\n","      # print('%s:%s\\t%s\\t(%s, %s)' % (str(sp1), str(sp2), str(span_pred), str(span_prob[0]), str(span_prob[1])))\n","\n","      if span_pred == 1:\n","        if pred_entailment == 1:\n","          for sp3 in range(sp1+1):\n","            if not pred_consistent:\n","              break\n","\n","            for sp4 in range(sp2, ex['length']):\n","              if not pred_consistent:\n","                break\n","\n","              sspan_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp3), str(sp4))]\n","\n","              if sspan_pred == 0:\n","                pred_consistent = False\n","                break\n","        elif pred_entailment == 0:\n","          pred_consistent = False\n","\n","  preds_consistent.append(1 if pred_consistent else 0)\n","  span_accuracies.append(span_accuracy / no_spans)\n","  span_accuracies_strict.append(span_accuracy_strict / no_spans)\n","  preds_coherent.append(1 if pred_coherent else 0)\n","\n","  # Check pred. breakpoint (verifiability) - will be first sentence where the model prediction becomes polarized, i.e., confidence > threshold\n","  pred_breakpoint = 0 # For now, 0 means -1, i.e., stories are entirely plausible - this shouldn't happen but it will (inconsistent?)\n","  for ss in range(1, ex['length']):\n","    if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n","      pred_breakpoint = ss\n","      break\n","  preds_breakpoint.append(pred_breakpoint)\n","\n","  # Check pred. evidence (verifiability)\n","  if pred_breakpoint > 0:\n","    pred_evidence = -1\n","    for ss in range(0, pred_breakpoint+1):\n","      if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n","        pred_evidence = ss\n","  else:\n","    pred_evidence = -1 # This should never happen - it would be inconsistent if it did\n","  preds_evidence.append(pred_evidence)\n","\n","# Calculate tiered accuracy for model\n","acc = 0\n","acc_con = 0\n","acc_con_vbp = 0\n","acc_con_vbp_vev = 0\n","no_ex = len(ids_base)\n","for p_plaus, l_plaus, con, p_bp, l_bp, p_ev, l_ev in zip(preds_entailment, labels_entailment, preds_consistent, preds_breakpoint, labels_breakpoint, preds_evidence, labels_evidence):\n","  # Accuracy\n","  if p_plaus == l_plaus:\n","    acc += 1\n","\n","    # Consistency\n","    if con == 1:\n","      acc_con += 1\n","\n","      # Verifiability (breakpoint)\n","      if p_bp == l_bp:\n","        acc_con_vbp += 1\n","\n","        # Verifiability (evidence)\n","        if p_ev == l_ev:\n","          acc_con_vbp_vev += 1\n","\n","acc /= no_ex\n","acc_con /= no_ex\n","acc_con_vbp /= no_ex\n","acc_con_vbp_vev /= no_ex\n","\n","# all_results['acc'] = acc\n","# all_results['acc_con'] = acc_con\n","# all_results['acc_con_vbp'] = acc_con_vbp\n","# all_results['acc_con_vbp_vev'] = acc_con_vbp_vev\n","# all_results['span_accuracy'] = np.mean(span_accuracies)\n","\n","all_results['lenient_coherence'] = np.mean(span_accuracies_strict)\n","all_results['strict_coherence'] = np.mean(preds_coherent)\n","\n","best_preds_entailment = preds_entailment\n","best_preds_consistent = preds_consistent\n","best_preds_breakpoint = preds_breakpoint\n","best_preds_evidence = preds_evidence\n","best_preds_coherent = preds_coherent\n","\n","print('\\nPARTITION: %s' % p)\n","print_dict(all_results)\n","\n","# Save preds for breakpoint and evidence\n","save_preds(ids_base, np.array(labels_breakpoint), best_preds_breakpoint, probe_model, p_dataset_name_bp)\n","save_preds(ids_base, np.array(labels_evidence), best_preds_evidence, probe_model, p_dataset_name_ev)\n","save_preds(ids_base, np.array([1 for p in best_preds_coherent]), best_preds_coherent, probe_model, p_dataset_name_coh)\n","\n","p_dataset_name_agg = '%s_tiers_agg_nostates_lenient_%s' % ('ConvEnt', p)\n","save_results(all_results, probe_model, p_dataset_name_agg)"],"metadata":{"id":"GxT_pHmgvlo1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"72078f03-eefe-4c5f-d7b1-1c46fa0bc91c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing model: drive/My Drive/CSE 595/FinalProject/saved_models/bert-large-uncased_ConvEnt_32_7.5e-06_7_xval.\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:05s.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["\n","PARTITION: test\n","{\n","  lenient_coherence: \n","    0.33255912653997466,\n","  strict_coherence: \n","    0.2441860465116279,\n","}\n","\n","\n"]}]},{"cell_type":"markdown","source":["#BERT + Graph Encodings"],"metadata":{"id":"YEPlg76TCqaZ"}},{"cell_type":"markdown","source":["## Load Dataset"],"metadata":{"id":"QpCf2gGk9RuQ"}},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\n","import pickle\n","cache_train = os.path.join(GOOGLE_DRIVE_PATH, 'data/ConvEnt_train_resplit.json')\n","cache_dev = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_resplit.json')\n","cache_test = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_test_resplit.json')\n","ConvEnt_train = json.load(open(cache_train))\n","ConvEnt_dev = json.load(open(cache_dev))\n","ConvEnt_test = json.load(open(cache_test))\n","\n","# Combine train and dev and do cross-validation\n","cache_folds = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_folds.pkl') # Folds used for results presented in paper\n","ConvEnt_train = ConvEnt_train + ConvEnt_dev\n","train_sources = list(set([ex['dialog_source'] for ex in ConvEnt_train]))\n","print(\"Reserved %s dialog sources for training and validation.\" % len(train_sources))\n","\n","no_folds = 8\n","if not os.path.exists(cache_folds):\n","  folds = []\n","  for k in range(no_folds):\n","    folds.append(np.random.choice(train_sources, size=5, replace=False))\n","    train_sources = [s for s in train_sources if s not in folds[-1]]\n","  assert len(train_sources) == 0\n","  print(folds)\n","  pickle.dump(folds, open(cache_folds, 'wb'))\n","else:\n","  folds = pickle.load(open(cache_folds, 'rb'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0jQVkR19Uwb","executionInfo":{"status":"ok","timestamp":1734116172340,"user_tz":300,"elapsed":1105,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"20bdcb3b-5e37-4b01-c730-3ed57f446dc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reserved 40 dialog sources for training and validation.\n"]}]},{"cell_type":"code","source":["print('train examples:', len(ConvEnt_train))\n","print('dev examples:', len(ConvEnt_dev))\n","print('test examples:', len(ConvEnt_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ni668D1E9ekR","executionInfo":{"status":"ok","timestamp":1734116172340,"user_tz":300,"elapsed":3,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"31392aea-6628-4df2-d8c5-628e2bb6a8b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train examples: 703\n","dev examples: 110\n","test examples: 172\n"]}]},{"cell_type":"code","source":["# if debug:\n","#   ConvEnt_train = ConvEnt_train[:10]\n","#   ConvEnt_dev = ConvEnt_dev[:10]\n","#   ConvEnt_test = ConvEnt_test[:10]"],"metadata":{"id":"WvKXMqWwc_1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print('train examples:', len(ConvEnt_train))\n","# print('dev examples:', len(ConvEnt_dev))\n","# print('test examples:', len(ConvEnt_test))"],"metadata":{"id":"OgU4D4HodDAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Named Entity Recognition"],"metadata":{"id":"rGC_QNsz810Q"}},{"cell_type":"code","source":["def coref_resolve(text):\n","    nlp1 = spacy.load('en_core_web_trf')\n","    nlp1.add_pipe('coreferee')\n","    doc1 = nlp1(text)\n","    tok_list = list(token.text for token in doc1)\n","    c = 0\n","    for chain in doc1._.coref_chains:\n","        for mention in chain:\n","            res1 = [doc1._.coref_chains.resolve(doc1[i]) for i in mention]\n","            res = list(filter((None).__ne__, res1))\n","            if len(res) != 0:\n","                if len(res[0]) == 1:\n","                    tok_list[mention[0] + c] = str(res[0][0])\n","                elif len(res[0]) > 1:\n","                    tok_list[mention[0] + c] = str(res[0][0])\n","                    for j in range(1, len(res[0])):\n","                        tok_list.insert(mention[0] + c + j, str(res[0][j]))\n","                        c = c + 1\n","    textres = \" \".join(tok_list)\n","    return textres\n","\n","\n","def extract_entities_and_relationships(text):\n","    doc = nlp(text)\n","    entities = [(ent.text, ent.label_) for ent in doc.ents]\n","    relationships = []\n","    for sent in doc.sents:\n","      # Iterate through the named entities (people, organizations etc.) in the sentence\n","      for ent in sent.ents:\n","          # Check if the entity has a known label and is a person or organization\n","          if ent.label_ in [\"PERSON\", \"ORG\"]:\n","              # Extract the relationship\n","              for token in sent:\n","                  if token.dep_ in [\"attr\", \"nsubj\", \"dobj\"]:\n","                      relationships.append((ent.text, token.text))\n","\n","    #Disregard entitity labels\n","    entities = [ent[0] for ent in entities]\n","\n","    return entities, relationships"],"metadata":{"id":"uEB7TxAX805D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"John Doe is a software engineer at XYZ Corporation. He has a degree in Computer Science from ABC University.\"\n","sentence = coref_resolve(sentence)\n","# sentence = \"SpeakerB : And , uh , SpeakerB was about , the , the piece of music , the piece of music was about , I think about forty or fifty years old .   And , SpeakerB was incredible , I mean , the parallel , you know , between piece and rap . SpeakerA : Right . SpeakerB : And , um , you , you listen a lot , if you , if you hear a lot of old gospel , uh , uh , especially well , the black gospel .   You know , you will , you know , you can really pick gospel up .   I mean ,\"\n","print(sentence)\n","nlp = spacy.load('en_core_web_sm')\n","\n","res_entities, res_relationships = extract_entities_and_relationships(sentence)\n","print(\"Entities:\", res_entities)\n","print(\"Relationships:\", res_relationships)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Phz46yfh9B1T","executionInfo":{"status":"ok","timestamp":1734116180229,"user_tz":300,"elapsed":7713,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"ad59d56a-9f44-4d8f-ad43-6616e59642c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self._model.load_state_dict(torch.load(filelike, map_location=device))\n","/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(self._mixed_precision):\n","<ipython-input-21-1f102c22b1db>:10: DeprecationWarning: NotImplemented should not be used in a boolean context\n","  res = list(filter((None).__ne__, res1))\n"]},{"output_type":"stream","name":"stdout","text":["John Doe is a software engineer at XYZ Corporation . Doe has a degree in Computer Science from ABC University .\n","Entities: ['John Doe', 'XYZ Corporation', 'Computer Science', 'ABC University']\n","Relationships: [('John Doe', 'Doe'), ('John Doe', 'engineer'), ('XYZ Corporation', 'Doe'), ('XYZ Corporation', 'engineer'), ('Computer Science', 'Doe'), ('Computer Science', 'degree'), ('ABC University', 'Doe'), ('ABC University', 'degree')]\n"]}]},{"cell_type":"markdown","source":["##Get Instanse-Specific Knowledge Graph by Querying Global Knowledge Graph"],"metadata":{"id":"Aym5k9bGCynK"}},{"cell_type":"code","source":["!pip install SPARQLWrapper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjVfqDchDksy","executionInfo":{"status":"ok","timestamp":1734116183594,"user_tz":300,"elapsed":3367,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"40c778b1-a4ef-4d6d-9695-8bb055b0a4c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting SPARQLWrapper\n","  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n","  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n","Collecting isodate<1.0.0,>=0.7.2 (from rdflib>=6.1.1->SPARQLWrapper)\n","  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.0)\n","Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n","Downloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: isodate, rdflib, SPARQLWrapper\n","Successfully installed SPARQLWrapper-2.0.0 isodate-0.7.2 rdflib-7.1.1\n"]}]},{"cell_type":"code","source":["from SPARQLWrapper import SPARQLWrapper, JSON\n","\n","# DBpedia SPARQL endpoint\n","sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n","\n","# Function to query DBpedia for information about an entity\n","def query_dbpedia(entity):\n","    query = f\"\"\"\n","    SELECT ?subject ?predicate ?object\n","    WHERE {{\n","      ?subject rdfs:label \"{entity}\"@en .\n","      ?subject ?predicate ?object\n","    }}\n","    LIMIT 50\n","    \"\"\"\n","    sparql.setQuery(query)\n","    sparql.setReturnFormat(JSON)\n","    results = sparql.query().convert()\n","    return results"],"metadata":{"id":"xGGlvmK1DpRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to build an instance-specific knowledge graph\n","def build_instance_kg(entities):\n","    kg = {}\n","    for entity in entities:\n","        results = query_dbpedia(entity)\n","        triples = [(result[\"subject\"][\"value\"], result[\"predicate\"][\"value\"], result[\"object\"][\"value\"])\n","                   for result in results[\"results\"][\"bindings\"]]\n","        kg[entity] = triples\n","    return kg"],"metadata":{"id":"aLaIViTDDzXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example entities to query\n","entities = res_entities\n","\n","# Build the knowledge graph\n","instance_kg = build_instance_kg(entities)\n","\n","# Print the knowledge graph\n","for entity, triples in instance_kg.items():\n","    print(f\"Entity: {entity}\")\n","    for subject, predicate, obj in triples:\n","        print(f\"  {subject} -> {predicate} -> {obj}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7J40OitBD7wa","executionInfo":{"status":"ok","timestamp":1734116186613,"user_tz":300,"elapsed":2942,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"d7954f79-7595-44cb-a127-054a7ef37b57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entity: John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/ontology/Person\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/WikicatAnonymityPseudonyms\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/WikicatCollectivePseudonyms\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/WikicatLanguageVarietiesAndStyles\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Ability105616246\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Abstraction100002137\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Assortment108398773\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Cognition100023271\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Collection107951464\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Creativity105624700\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/FictionalCharacter109587565\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Group100031264\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/ImaginaryBeing109483738\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Imagination105625465\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/LanguageUnit106284225\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Name106333653\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Part113809207\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Pseudonym106338278\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/PsychologicalFeature100023100\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/Relation100031921\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/WikicatFictionalCharacters\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/class/yago/WikicatPseudonyms\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> جون دو\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> 존 도\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John e Jane Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> Джон Доу\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> Джон Доу\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#label -> John Doe\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> «جون دو» أو «جون رو» أو «ريتشارد رو» (للرجال) و«جين دو» أو «جين رو» (للنساء) و«بيبي دو» أو «جاني دو» أو «جوني دوي» (للأطفال)، أو فقط «دو» أو «رو» (بالإنجليزية: \"John Doe\" , \"John Roe\" or \"Richard Roe\" (for men), \"Jane Doe\" or \"Jane Roe\" (for women), and \"Baby Doe\", \"Janie Doe\" or \"Johnny Doe\" (for children), or just \"Doe\" or \"Roe\")‏ اسم مستعار يستعمل في اللغة الإنجليزية (الأمريكية خاصة) لدلالة على شخص في المطلق أو لتسمية أطراف في نزاع قانوني أو كاسم تخفي لبعض الشهود المهددين عند ذكر شهاداتهم أو لعدم معرفة هوية الشخص. تترجم أحيانا إلى العربية ب «فلان الفلاني».\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe, weibliches Pendant Jane Doe oder Jane Roe, ist ein englischer Platzhaltername für fiktive oder nicht identifizierte Personen. John ist einer der häufigsten englischen Männernamen. „Doe“ bezeichnet eine Hirschkuh, „Roe“ ein Reh. Der Begriff ist vor allem in den Vereinigten Staaten gebräuchlich.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe (gizasemea) eta Jane Doe (emakumezkoa) pertsona baten benetako izena ezezaguna denean edo nahita ezkutatzen denean ingelesez erabiltzen diren izenak dira. Ingelesezko tradiziotik nazioarteko hainbat kulturetara hedatutako izena, gizaseme ezezaguna izendatzeko erabili ohi den izena da John Doe. Legea Estatu Batuetan aplikatzeko testuinguruan, izen horiek erabiltzen dira, maiz, identitatea ezagutzen ez den edo berresten ez den gorpu bat izendatzeko. Izen horiek maiz erabiltzen dira, beste testuinguru batzuetan, \"edozein pertsona\" hipotetikoa izendatzeko, John Q. Public edo Joe Public izenak bezala. Aurreko izen horien aldaera asko daude: John Roe, Richard Roe, Jane Roe, Baby Doe eta Janie Doe/Johnny Doe (haurrentzat).\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe (male) and Jane Doe (female) are multiple-use placeholder names that are used when the true name of a person is unknown or is being intentionally concealed. In the context of law enforcement in the United States, such names are often used to refer to a corpse whose identity is unknown or unconfirmed. These names are also often used to refer to a hypothetical \"everyman\" in other contexts, in a manner similar to John Q. Public or \"Joe Public\". There are many variants to the above names, including John Roe, Richard Roe, Jane Roe, Baby Doe, and Janie Doe/Johnny Doe (for children).\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> En anglais, John Doe (version féminine : Jane Doe) est une expression qui désigne une personne non identifiée ou un homme de la rue, soit en français : « Monsieur X », « Monsieur Untel », « Monsieur Durand », « Monsieur Dupont », « Monsieur Tout-le-monde », « un citoyen Lambda ». Pour les très jeunes enfants, l'équivalent est « Baby Doe », ou encore « Jonnie Doe » ou « Janie Doe » quand on précise le sexe.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> ( 다른 뜻에 대해서는 존 도 (동음이의) 문서를 참고하십시오.)( 제인 도는 여기로 연결됩니다. 영화에 대해서는 제인 도 (영화) 문서를 참고하십시오.)\n","존 도(영어: John Doe)는 이름이 없는(공개되지 않은) 사람을 가리켜 사용하는 영어식 인명이다. 한국어의 경우 홍길동이나 아무개에 해당한다. 여성 이름으로 제인 도(Jane Doe) 또는 제인 로(Jane Roe)가 쓰인다. 존 스미스(John Smith)와 제인 스미스(Jane Smith) 역시 많이 쓰인다.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe è un nome usato solitamente nel gergo giuridico statunitense per indicare un uomo la cui reale identità è sconosciuta o va mantenuta tale. Il suo equivalente femminile è Jane Doe, mentre nel caso di bambini è frequente l'uso di Baby Doe, anche se sono utilizzati i nomi Johnny Doe per i maschi e Janie Doe per le femmine.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe – nazwisko używane w USA dla określenia mężczyzny o niezidentyfikowanej lub ukrytej tożsamości. Dla określenia kobiet używa się nazwiska Jane Doe. Termin ten stosuje się zwłaszcza w aktach policyjnych, sądownictwie i dokumentach prawniczych. W innym znaczeniu „John Doe” to także zwykły, szary obywatel – porównać go można z polskim Janem Kowalskim, chociaż w tym kontekście używane są także nazwiska John Q. Public.W Polsce odpowiednikiem opisywanego pojęcia w terminologii prawniczej i policyjnej jest skrót N.N.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> \"John Doe\" (para homens) e \"Jane Doe\" (para mulheres) são que são usados quando o nome verdadeiro de uma pessoa é desconhecido ou está sendo intencionalmente oculto. No contexto da , esses nomes são frequentemente usados para se referir a um cadáver cuja identidade é desconhecida ou não confirmada. Em segundo lugar, esses nomes também são frequentemente usados para se referir a um hipotético everyman (homem comum) em outros contextos, de maneira semelhante a \"John Q. Public\" ou \"Joe Public\". Existem muitas variantes para os nomes acima, incluindo \"John Roe\", \"Richard Roe\", \"Jane Roe\" e \"Baby Doe\", \"Janie Doe\" ou \"Johnny Doe\" (para crianças).\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe（用于男性）和Jane Doe（用于女性）是一个，當某人的真實姓名未知或被故意隱藏時使用。在美國警察用语中，这种名称常用来指身份不明或未经确认的屍體。其次，其他语境中这种名字也常用来指代一个假想的，类似于或Joe Public。上述名字也有诸多变种，包括John Roe、Richard Roe、Jane Roe，以及用于儿童的Baby Doe、Janie Doe和Johnny Doe。\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> \"John Doe\" untuk laki-laki atau \"Jane Doe\" untuk perempuan, adalah nama fiktif yang dipakai untuk terdakwa yang nama aslinya tidak diketahui pada saat gugatan disampaikan dalam suatu perkara hukum. Nama ini juga dipakai sebagai nama fiktif untuk jenazah yang ditemukan tanpa identitas/belum teridentifikasi, atau orang tanpa identitas yang dirawat di rumah sakit. Praktik ini dipakai secara luas di Amerika Serikat dan Kanada, tetapi jarang dipakai di negara-negara berbahasa Inggris lainnya, termasuk Britania Raya, negara asal pemakaian nama \"John Doe\" dalam konteks hukum.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> John Doe en Jane Doe, ook wel John Roe en Jane Roe, zijn pantoniemen die in Engelstalige gebieden (vooral in de Verenigde Staten) worden gebruikt om een anoniem persoon aan te duiden. Het wordt bijvoorbeeld gebruikt voor de verdachte/gedaagde of het slachtoffer in een juridische tekst, om de identiteit van de betreffende persoon te beschermen. Maar het kan ook gebruikt worden als personificatie van 'een willekeurig persoon'. Meet John Doe is een Amerikaanse film uit 1941 van Frank Capra met Gary Cooper in de rol van de voormalige baseballspeler en zwerver John Doe alias Long John Willoughby.\n","  http://dbpedia.org/resource/John_Doe -> http://www.w3.org/2000/01/rdf-schema#comment -> Джон До́у (англ. John Doe) — экземплификант, обозначение мужской стороны в судебном процессе (англосаксонское право). Возможно, заимствован из разработанных римскими юристами типовых процессуальных формул (см. «Институции» Гая), в которых истец носил условное имя Авл Агерий, а ответчик — . В настоящее время часто используется в англоязычных СМИ для обозначения анонимного или малозначимого персонажа (ср. Иван Иванович Иванов, Вася Пупкин).\n","\n","Entity: XYZ Corporation\n","\n","Entity: Computer Science\n","  http://dbpedia.org/resource/Computer_Science -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://dbpedia.org/ontology/University\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://www.w3.org/1999/02/22-rdf-syntax-ns#type -> http://www.w3.org/2004/02/skos/core#Concept\n","  http://dbpedia.org/resource/Computer_Science -> http://www.w3.org/2000/01/rdf-schema#label -> Computer Science\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://www.w3.org/2000/01/rdf-schema#label -> Computer Science\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://www.w3.org/2004/02/skos/core#prefLabel -> Computer Science\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/ontology/wikiPageID -> 5983\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://dbpedia.org/ontology/wikiPageID -> 11391882\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/ontology/wikiPageRevisionID -> 824154073\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://dbpedia.org/ontology/wikiPageRevisionID -> 561411903\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/ontology/wikiPageWikiLink -> http://dbpedia.org/resource/Computer_science\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/ontology/wikiPageRedirects -> http://dbpedia.org/resource/Computer_science\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/property/wikiPageUsesTemplate -> http://dbpedia.org/resource/Template:R_from_capitalization\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/property/wikiPageUsesTemplate -> http://dbpedia.org/resource/Template:Redirect_category_shell\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://dbpedia.org/property/wikiPageUsesTemplate -> http://dbpedia.org/resource/Template:Category_redirect\n","  http://dbpedia.org/resource/Computer_Science -> http://www.w3.org/ns/prov#wasDerivedFrom -> http://en.wikipedia.org/wiki/Computer_Science?oldid=824154073&ns=0\n","  http://dbpedia.org/resource/Category:Computer_Science -> http://www.w3.org/ns/prov#wasDerivedFrom -> http://en.wikipedia.org/wiki/Category:Computer_Science?oldid=561411903&ns=14\n","  http://dbpedia.org/resource/Computer_Science -> http://dbpedia.org/ontology/wikiPageLength -> 89\n","  http://dbpedia.org/resource/Computer_Science -> http://xmlns.com/foaf/0.1/isPrimaryTopicOf -> http://en.wikipedia.org/wiki/Computer_Science\n","\n","Entity: ABC University\n","\n"]}]},{"cell_type":"code","source":["for value in instance_kg.values():\n","  print(len(value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QEewRwZGVrGo","executionInfo":{"status":"ok","timestamp":1734116186703,"user_tz":300,"elapsed":92,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"f47e51d1-6ce0-49b3-a562-d72e7a8ef2e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","0\n","18\n","0\n"]}]},{"cell_type":"markdown","source":["##Generate Node Embeddings From Instance-Specific Knowledge Graph"],"metadata":{"id":"38r_oLcIFmvb"}},{"cell_type":"code","source":["!pip install node2vec networkx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d06fFeqFvXo","executionInfo":{"status":"ok","timestamp":1734116189466,"user_tz":300,"elapsed":2764,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"77fb15fa-dc8a-496b-c680-b6bfea037fb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting node2vec\n","  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n","Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n","Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n","Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.66.6)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (6.4.0)\n","Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n","Installing collected packages: node2vec\n","Successfully installed node2vec-0.5.0\n"]}]},{"cell_type":"code","source":["import networkx as nx\n","from node2vec import Node2Vec\n","\n","# Construct a NetworkX graph from the instance-specific knowledge graph\n","def construct_graph(instance_kg, relationships):\n","    G = nx.DiGraph()\n","    for entity, triples in instance_kg.items():\n","        for subject, predicate, obj in triples:\n","            G.add_node(entity, label=entity)\n","            G.add_node(obj)\n","            G.add_edge(subject, obj, label=predicate)\n","    for rel in relationships:\n","        G.add_node(rel[0], label=rel[0])\n","        G.add_node(rel[1], label=rel[1])\n","        G.add_edge(rel[0], rel[1])\n","    return G\n","\n","# Generate node embeddings using node2vec\n","def generate_node_embeddings(G, entity_nodes, dimensions=768, walk_length=30, num_walks=200, workers=4):\n","    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)\n","\n","    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n","\n","    # Retrieve embeddings only for the entity nodes\n","    node_embeddings = {str(node): model.wv[str(node)] for node in entity_nodes if str(node) in model.wv}\n","    return node_embeddings\n"],"metadata":{"id":"ke8AcwVYFlhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relationships = res_relationships\n","\n","# Construct the graph\n","G = construct_graph(instance_kg, relationships)\n","\n","# Generate node embeddings only for the entity nodes\n","node_embeddings = generate_node_embeddings(G, entities)\n","\n","# Print embeddings of the entity nodes\n","for node, embedding in node_embeddings.items():\n","    print(f\"Node: {node}, Embedding: {embedding[:5]}...\")  # Print only the first 5 dimensions for brevity"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["0db5780818ef411abb02f25b51fa7a13","859765efe33b4fe8a5de94fcb958c29f","562beb6fcaee41898c1376cc49f2a731","32858879253d44a88af6740ac2d8709f","ad29a2bde1b14c0eb3e83caa3480cc9c","198822e942f349f7b44b60f2272b0886","893ce291e9584876878ffbd669f1d067","46ab7eb538d742ffb00ab399be1abdca","0be23950e451453db9880a729c3621c3","92a4a069f0be4e28a2250cad5985ec66","721409a85bd74008ae38e66bb522f33e"]},"id":"2NLhYWtMF4Iu","executionInfo":{"status":"ok","timestamp":1734116196728,"user_tz":300,"elapsed":4721,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"f4d40d42-3236-4716-dafb-837fc9809e53"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Computing transition probabilities:   0%|          | 0/63 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db5780818ef411abb02f25b51fa7a13"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Node: John Doe, Embedding: [-7.1464196e-06  9.6979871e-04 -2.6296737e-04 -2.8108250e-04\n","  1.0065751e-03]...\n","Node: XYZ Corporation, Embedding: [ 0.0004201  -0.00075284  0.00056785  0.00120331 -0.00107555]...\n","Node: Computer Science, Embedding: [-0.00133304 -0.00013657 -0.0011246   0.00019095  0.00033662]...\n","Node: ABC University, Embedding: [-2.2274052e-04  1.1672511e-03 -4.4540322e-04  8.5892412e-04\n","  6.8043759e-05]...\n"]}]},{"cell_type":"markdown","source":["## Featurize BERT Embeddings with Node Embeddings"],"metadata":{"id":"yWThBQc9R7XZ"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset\n","import spacy\n","import progressbar\n","import numpy as np\n","\n","# Creates tensor dataset from featurized dataset\n","def get_tensor_dataset_plus_node(dataset, label_key='plausible', add_segment_ids=False):\n","  # print(len(dataset[0]['input_ids']))\n","  # print(len(dataset[1]['input_ids']))\n","  all_input_ids = torch.tensor([ex['input_ids'] for ex in dataset])\n","  all_input_node_embeddings_idx = torch.tensor([ex['input_node_embeddings_idx'] for ex in dataset])\n","  all_input_mask = torch.tensor([ex['input_mask'] for ex in dataset])\n","  all_label_ids = torch.tensor([ex[label_key] for ex in dataset], dtype=torch.long)\n","\n","  if 'segment_ids' in dataset[0]:\n","    all_segment_ids = torch.tensor([ex['segment_ids'] for ex in dataset])\n","    tensor_dataset = TensorDataset(all_input_ids, all_input_node_embeddings_idx, all_input_mask, all_label_ids, all_segment_ids)\n","  else:\n","    tensor_dataset = TensorDataset(all_input_ids, all_input_node_embeddings_idx, all_input_mask, all_label_ids)\n","  return tensor_dataset\n","\n","# Tokenize, numericalize, and pad an input dataset\n","def add_bert_features_ConvEnt_plus_node(dataset, tokenizer, seq_length, node_embeddings_from_storage=False, node_embeddings_path=None, add_segment_ids=False):\n","\n","  # Get stored node embeddings if available\n","  if node_embeddings_from_storage:\n","    print('Loading node embeddings from storage...')\n","    node_embeddings_dict = pickle.load(open(node_embeddings_path, 'rb'))\n","  else:\n","    node_embeddings_dict = {}\n","\n","  for i, ex in enumerate(dataset):\n","    exid = ex['id']\n","    ex['example_id'] = exid\n","\n","    # Numericalize the data\n","    dialog = ' '.join(['Speaker%s: %s' % (turn['speaker'], turn['text']) for turn in ex['turns']])\n","    inputs = tokenizer.encode_plus(dialog,\n","                                    text_pair=ex['hypothesis'],\n","                                    add_special_tokens=True,\n","                                    max_length=seq_length,\n","                                    truncation=True)\n","    input_ids = inputs['input_ids']\n","    if 'token_type_ids' in inputs:\n","      token_type_ids = inputs['token_type_ids']\n","\n","    if not node_embeddings_from_storage:\n","      # Coreference dialog for better entity/relationship extraction\n","      coref_dialog = coref_resolve(dialog)\n","\n","      # Entity/relationship extracton\n","      entities, relationships = extract_entities_and_relationships(coref_dialog)\n","\n","      if(len(entities) > 0):\n","\n","        # Build the local knowledge graph\n","        instance_kg = build_instance_kg(entities)\n","\n","        relevant_entities = False\n","        for value in instance_kg.values():\n","          if len(value) > 0:\n","            relevant_entities = True\n","            break\n","\n","        if len(relationships) > 0:\n","          relevant_entities = True\n","\n","        if relevant_entities:\n","\n","          print(exid, len(entities))\n","          print(coref_dialog)\n","\n","          # Construct the graph\n","          G = construct_graph(instance_kg, relationships)\n","\n","          # Generate node embeddings only for the entity nodes\n","          node_embeddings = generate_node_embeddings(G, entities)\n","\n","        else:\n","          node_embeddings = {}\n","\n","      else:\n","\n","        node_embeddings = {}\n","\n","      # Store node embeddings in dictionary, so that node embeddings do not have to be recreated\n","      node_embeddings_dict[i] = node_embeddings\n","\n","    # Don't want to truncate any data\n","    assert not('num_truncated_tokens' in inputs and inputs['num_truncated_tokens'] > 0)\n","    assert len(input_ids) <= seq_length\n","\n","    # Pad to sequence length of 128\n","    padding_length = seq_length - len(input_ids)\n","    input_length = len(input_ids)\n","    input_ids = input_ids + ([0] * padding_length)\n","    attention_mask = [1] * input_length + ([0] * padding_length) # Mask will zero out padding tokens\n","    if 'token_type_ids' in inputs:\n","      token_type_ids = token_type_ids + [0] * padding_length\n","\n","    assert len(input_ids) == len(attention_mask) == seq_length\n","\n","    dataset[i]['input_ids'] = input_ids\n","    dataset[i]['input_mask'] = attention_mask\n","    if 'token_type_ids' in inputs:\n","      dataset[i]['segment_ids'] = token_type_ids\n","    dataset[i]['input_node_embeddings_idx'] = [i] * seq_length\n","\n","  return dataset, node_embeddings_dict\n"],"metadata":{"id":"EJ0wozzWSEr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","seq_length = 128\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","cache_embeddings_train = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_node_embeddings_train.pkl')\n","cache_embeddings_dev = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_node_embeddings_dev.pkl')\n","cache_embeddings_test = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_node_embeddings_test.pkl')\n","\n","# ConvEnt_train, node_embeddings_train = add_bert_features_ConvEnt_plus_node(ConvEnt_train, tokenizer, seq_length, add_segment_ids=True)\n","ConvEnt_dev, node_embeddings_dev = add_bert_features_ConvEnt_plus_node(ConvEnt_dev, tokenizer, seq_length, add_segment_ids=True, node_embeddings_from_storage=True, node_embeddings_path=cache_embeddings_dev)\n","\n","# Cache node_embeddings_dev\n","if not os.path.exists(cache_embeddings_dev):\n","  print('Caching node embeddings...')\n","  # print(node_embeddings_dev)\n","  pickle.dump(node_embeddings_dev, open(cache_embeddings_dev, 'wb'))\n","\n","ConvEnt_test, node_embeddings_test = add_bert_features_ConvEnt_plus_node(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True, node_embeddings_from_storage=True, node_embeddings_path=cache_embeddings_test)\n","\n","# Cache node_embeddings_test\n","if not os.path.exists(cache_embeddings_test):\n","  print('Caching node embeddings...')\n","  # print(node_embeddings_test)\n","  pickle.dump(node_embeddings_test, open(cache_embeddings_test, 'wb'))\n","\n","ConvEnt_train, node_embeddings_train = add_bert_features_ConvEnt_plus_node(ConvEnt_train, tokenizer, seq_length, add_segment_ids=True, node_embeddings_from_storage=True, node_embeddings_path=cache_embeddings_train)\n","\n","# Cache node_embeddings_train\n","if not os.path.exists(cache_embeddings_train):\n","  print('Caching node embeddings...')\n","  # print(node_embeddings_train)\n","  pickle.dump(node_embeddings_train, open(cache_embeddings_train, 'wb'))\n","\n","\n","ConvEnt_train_folds = [[] for _ in range(no_folds)]\n","ConvEnt_dev_folds = [[] for _ in range(no_folds)]\n","for k in range(no_folds):\n","  ConvEnt_train_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] not in folds[k]]\n","  ConvEnt_dev_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] in folds[k]]\n","\n","  if debug:\n","    ConvEnt_train_folds[k] = ConvEnt_train_folds[k][:10]\n","    ConvEnt_dev_folds[k] = ConvEnt_dev_folds[k][:10]\n","\n","if debug:\n","  ConvEnt_train = ConvEnt_train[:10]\n","  ConvEnt_dev = ConvEnt_dev[:10]\n","  ConvEnt_test = ConvEnt_test[:10]\n","\n","ConvEnt_train_tensor = get_tensor_dataset_plus_node(ConvEnt_train, label_key='label', add_segment_ids=True)\n","ConvEnt_test_tensor = get_tensor_dataset_plus_node(ConvEnt_test, label_key='label', add_segment_ids=True)\n","\n","# Training sets for each validation fold\n","ConvEnt_train_folds_tensor = [get_tensor_dataset_plus_node(ConvEnt_train_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]\n","ConvEnt_dev_folds_tensor = [get_tensor_dataset_plus_node(ConvEnt_dev_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-tZXJ4F889e","executionInfo":{"status":"ok","timestamp":1734116200969,"user_tz":300,"elapsed":4159,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"c05b988a-82db-48eb-c13c-e3ccdadb933b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading node embeddings from storage...\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Loading node embeddings from storage...\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Loading node embeddings from storage...\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]}]},{"cell_type":"code","source":["print('train examples:', len(ConvEnt_train))\n","print('dev examples:', len(ConvEnt_dev))\n","print('test examples:', len(ConvEnt_test))"],"metadata":{"id":"ujHIZzBe-Pjz","executionInfo":{"status":"ok","timestamp":1734116210070,"user_tz":300,"elapsed":83,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"672b5f8d-768a-4fd4-82da-e33029ba6904"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train examples: 703\n","dev examples: 110\n","test examples: 172\n"]}]},{"cell_type":"code","source":["trainNodeEmbeddingCount = 0\n","for item in node_embeddings_train.values():\n","  if bool(item):\n","    trainNodeEmbeddingCount += 1\n","print(\"Percentage of train with node embeddings:\", trainNodeEmbeddingCount / len(node_embeddings_train))\n","\n","devNodeEmbeddingCount = 0\n","for item in node_embeddings_dev.values():\n","  if bool(item):\n","    devNodeEmbeddingCount += 1\n","print(\"Percentage of dev with node embeddings:\", devNodeEmbeddingCount / len(node_embeddings_dev))\n","\n","testNodeEmbeddingCount = 0\n","for item in node_embeddings_test.values():\n","  if bool(item):\n","    testNodeEmbeddingCount += 1\n","print(\"Percentage of test with node embeddings:\", testNodeEmbeddingCount / len(node_embeddings_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRA4iLJfX5Gz","executionInfo":{"status":"ok","timestamp":1734116442300,"user_tz":300,"elapsed":138,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"6b6418cc-4e35-4350-955d-704b01a26ede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of train with node embeddings: 0.3741109530583215\n","Percentage of dev with node embeddings: 0.2818181818181818\n","Percentage of test with node embeddings: 0.46511627906976744\n"]}]},{"cell_type":"markdown","source":["## Augment Embeddings"],"metadata":{"id":"h0v7_BsiioSs"}},{"cell_type":"code","source":["def augment_embedding(input_ids, token_embeddings, node_embeddings, tokenizer):\n","\n","  batch_size = input_ids.size(0)\n","  seq_length = input_ids.size(1)\n","  device = token_embeddings.device\n","\n","  # Initialize a list to hold the augmented embeddings\n","  augmented_embeddings = []\n","\n","  # Loop through each item in batch\n","  for i in range(batch_size):\n","      instance_embeddings = []\n","\n","      # Loop through each token in the input\n","      for token, token_emb in zip(input_ids[i], token_embeddings[i]):\n","          token_str = tokenizer.decode([token]).strip()\n","\n","          # Check if the token corresponds to an entity with a node embedding\n","          if token_str in node_embeddings:\n","              node_embedding = node_embeddings[token_str]\n","              combined_embedding = torch.cat((token_emb, node_embedding), dim=0)\n","          else:\n","              # If no corresponding node embedding, pad with zeros or use the original embedding\n","              node_embedding = torch.zeros(768, device=device)\n","              combined_embedding = torch.cat((token_emb, node_embedding), dim=0)\n","          instance_embeddings.append(combined_embedding)\n","\n","      # Add instance imbedding to batch\n","      instance_embeddings = torch.stack(instance_embeddings)\n","      augmented_embeddings.append(instance_embeddings)\n","\n","  # Convert list to tensor\n","  augmented_embeddings = torch.stack(augmented_embeddings)\n","\n","  return augmented_embeddings\n"],"metadata":{"id":"OGeVj0g4iugZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Augmented BERT"],"metadata":{"id":"ys-2wa_cfffG"}},{"cell_type":"code","source":["# Define a custom BERT model that handles augmented embeddings\n","class AugmentedBERTModel(torch.nn.Module):\n","    def __init__(self, bert_model_class, tokenizer, model_name, cache_dir=None): # Pass model name and class\n","        super(AugmentedBERTModel, self).__init__()\n","        # Load pretrained bert model\n","        self.bert_model = bert_model_class.from_pretrained(model_name, cache_dir=cache_dir)\n","        self.tokenizer = tokenizer\n","        self.fc = torch.nn.Linear(1792, 1024)  # Reduce dimensions back to 1024\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.to(self.device)  # Move the model to the device\n","\n","    def forward(self, input_ids, node_embeddings, attention_mask=None, token_type_ids=None, labels=None):\n","        # Get the token embeddings from BERT's embedding layer\n","        token_embeddings = self.bert_model.bert.embeddings(input_ids)\n","\n","        # print(input_ids.shape)\n","        # print(token_embeddings.shape)\n","\n","        augmented_embeddings = augment_embedding(input_ids, token_embeddings, node_embeddings, self.tokenizer)\n","\n","        # print(augmented_embeddings.shape)\n","\n","        # Pass the augmented embeddings through the initial layers\n","        hidden_states = self.fc(augmented_embeddings)\n","\n","        # Get the outputs from the BERT model\n","        outputs = self.bert_model(\n","            inputs_embeds=hidden_states,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            labels=labels\n","        )\n","        return outputs"],"metadata":{"id":"sYES2h6Ifd_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config_class = BertConfig\n","emb_class = BertModel\n","model_class = AugmentedBERTModel"],"metadata":{"id":"DfKH-WXZW5Kf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train & Evaluate Functions"],"metadata":{"id":"f_i69ZLm_Rd1"}},{"cell_type":"code","source":["import time\n","import torch\n","from www.utils import format_time\n","import numpy as np\n","from transformers import RobertaForMultipleChoice\n","import progressbar\n","from www.model.eval import evaluate_tiered\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Train a PyTorch model for one epoch\n","def train_epoch_plus_node(model, optimizer, train_dataloader, device, node_embedding_dict, list_output=False, num_outputs=1, span_mode=False, seg_mode=False, classifier=None, multitask_idx=None):\n","  t0 = time.time()\n","\n","  if not list_output:\n","    total_loss = 0\n","  else:\n","    total_loss = [0 for _ in range(num_outputs)]\n","\n","  # Training mode\n","  model.train()\n","\n","  if len(train_dataloader) * train_dataloader.batch_size >= 2500:\n","    progress_update = True\n","  else:\n","    progress_update = False\n","\n","  for step, batch in enumerate(train_dataloader):\n","    # Progress update\n","    if progress_update and step % 50 == 0 and not step == 0:\n","      elapsed = format_time(time.time() - t0)\n","      print('\\t(%s) Starting batch %s of %s.' % (elapsed, str(step), str(len(train_dataloader))))\n","\n","    input_ids = batch[0].to(device)\n","\n","    # Get node embedding index and move embeddings to device\n","    #TODO - NEED TO LOOP THROUGH ALL ITEMS IN BATCH, ADD ALL NODE EMEDDINGS\n","    input_node_embeddings_idx = batch[1][0][0].item()\n","    input_node_embeddings = node_embedding_dict[input_node_embeddings_idx]\n","\n","    # print(input_node_embeddings)\n","    for item in input_node_embeddings:\n","      input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n","\n","    input_mask = batch[2].to(device)\n","    labels = batch[3].to(device)\n","    segment_ids = batch[4].to(device)\n","\n","    # Forward pass\n","    model.zero_grad()\n","    out = model(input_ids,\n","                input_node_embeddings,\n","                token_type_ids=segment_ids,\n","                attention_mask=input_mask,\n","                labels=labels)\n","\n","    loss = out[0]\n","\n","    # Backward pass\n","    if not list_output:\n","      total_loss += loss.item()\n","      loss.backward()\n","    else:\n","      for o in range(num_outputs):\n","        total_loss[o] += loss[o].item()\n","        loss[o].backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n","\n","    optimizer.step()\n","\n","  if list_output:\n","    return list(np.array(total_loss) / len(train_dataloader)), model\n","  else:\n","    return total_loss / len(train_dataloader), model\n"],"metadata":{"id":"z8kJ5hQG_Yz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import time\n","from www.utils import format_time, read_tsv\n","import torch\n","import json\n","import os\n","import progressbar\n","from www.dataset.ann import att_default_values\n","\n","def compute_metrics(preds, labels, metrics):\n","  metr = {}\n","  for m, m_name in metrics:\n","    if m_name in ['accuracy', 'confusion_matrix']:\n","      metr[m_name] = m(labels, preds) # Assume each metric m will be a function of (y_true, y_pred)\n","    else:\n","      metr[m_name] = m(labels, preds, average='macro')\n","  return metr\n","\n","# Run evaluation for a PyTorch model\n","def evaluate_plus_node(model, eval_dataloader, device, node_embedding_dict, metrics, list_output=False, num_outputs=1, span_mode=False, seg_mode=False, return_softmax=False, multilabel=False, lm=False):\n","  print('\\tBeginning evaluation...')\n","\n","  t0 = time.time()\n","\n","  model.zero_grad()\n","  model.eval()\n","\n","  all_labels = None\n","  if not list_output:\n","    all_preds = None\n","    if return_softmax:\n","      all_logits = None\n","  else:\n","    all_preds = [np.array([]) for _ in range(num_outputs)]\n","\n","  print('\\t\\tRunning prediction...')\n","  # Get preds from model\n","  for batch in eval_dataloader:\n","\n","    # Move to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    input_ids, input_node_embeddings_idx, input_mask, labels, segment_ids = batch\n","\n","    # Get node embedding index and move embeddings to device\n","    #TODO - NEED TO LOOP THROUGH ALL ITEMS IN BATCH, ADD ALL NODE EMEDDINGS\n","    input_node_embeddings_idx = input_node_embeddings_idx[0][0].item()\n","    input_node_embeddings = node_embedding_dict[input_node_embeddings_idx]\n","\n","    for item in input_node_embeddings:\n","      input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n","\n","    input_node_embeddings = node_embedding_dict[input_node_embeddings_idx]\n","\n","    for item in input_node_embeddings:\n","      input_node_embeddings[item] = input_node_embeddings[item].to(device)\n","\n","    with torch.no_grad():\n","      out = model(input_ids,\n","                  input_node_embeddings,\n","                  token_type_ids=segment_ids,\n","                  attention_mask=input_mask)\n","\n","    label_ids = labels.to('cpu').numpy()\n","    if all_labels is None:\n","      all_labels = label_ids\n","    else:\n","      all_labels = np.concatenate((all_labels, label_ids), axis=0)\n","    # print(all_labels.shape)\n","\n","    logits = out[0]\n","    if list_output: # This is broken, do not use\n","      metr = {}\n","      for o in range(num_outputs):\n","        preds = torch.argmax(logits[o], dim=1).detach().cpu().numpy()\n","        all_preds[o] = np.concatenate((all_preds[o], preds))\n","        metr_o = compute_metrics(all_preds[o], all_labels, metrics)\n","        for k, v in metr_o.items():\n","          metr[str(o) + '_' + k] = v\n","    else:\n","      preds = torch.argmax(logits, dim=1 if not lm else 2).detach().cpu().numpy()\n","      # print(preds.shape)\n","      if all_preds is None:\n","        all_preds = preds\n","      else:\n","        all_preds = np.concatenate((all_preds, preds))\n","      # print(all_preds.shape)\n","      if return_softmax:\n","        logits = torch.softmax(logits, dim=1 if not lm else 2).detach().cpu().numpy()\n","        if all_logits is None:\n","          all_logits = logits\n","        else:\n","          all_logits = np.concatenate((all_logits, logits))\n","\n","  # Calculate metrics\n","  print('\\t\\tComputing metrics...')\n","  metr = compute_metrics(all_preds, all_labels, metrics)\n","\n","  print('\\tFinished evaluation in %ss.' % str(format_time(time.time() - t0)))\n","\n","  if not return_softmax:\n","    return metr, all_preds, all_labels\n","  else:\n","    # Warning: this is not supported in list_output mode\n","    return metr, all_preds, all_labels, all_logits"],"metadata":{"id":"cH5a1zuTBgw1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"NOWdCwm_-R6c"}},{"cell_type":"code","source":["batch_sizes = [config_batch_size]\n","learning_rates = [config_lr]\n","epochs = config_epochs\n","eval_batch_size = 128"],"metadata":{"id":"MssOU9g9TrNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.eval import save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","from collections import Counter\n","\n","seed_val = 22 # Save random seed for reproducibility\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","assert len(batch_sizes) == 1\n","train_fold_sampler = [RandomSampler(f) for f in ConvEnt_train_folds_tensor]\n","train_fold_dataloader = [DataLoader(f, sampler=train_fold_sampler[i], batch_size=batch_sizes[0]) for i, f in enumerate(ConvEnt_train_folds_tensor)]\n","\n","dev_fold_sampler = [SequentialSampler(f) for f in ConvEnt_dev_folds_tensor]\n","dev_fold_dataloader = [DataLoader(f, sampler=dev_fold_sampler[i], batch_size=eval_batch_size) for i, f in enumerate(ConvEnt_dev_folds_tensor)]\n","\n","all_val_accs = Counter()\n","print('Beginning grid search for ConvEnt over %s parameter combination(s)!' % (str(len(batch_sizes) * len(learning_rates))))\n","for bs in batch_sizes:\n","  for lr in learning_rates:\n","    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n","\n","    for k in range(no_folds):\n","      print('Beginning fold %s/%s...' % (str(k+1), str(no_folds)))\n","\n","      # Set up model\n","      if 'mnli' not in mode:\n","        model = model_class(BertForSequenceClassification, tokenizer, model_name, cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","        # model = model_class.from_pretrained(model_name, cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","      else:\n","        config = config_class.from_pretrained(model_name.replace('-mnli',''),\n","                                        num_labels=3,\n","                                        cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","        model = model_class.from_pretrained(model_name,\n","                                            config=config,\n","                                            cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","        config.num_labels = 2\n","        model.num_labels = 2\n","        model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n","\n","      model.cuda()\n","      device = model.device\n","\n","      # Set up optimizer\n","      optimizer = AdamW(model.parameters(), lr=lr)\n","      total_steps = len(train_fold_dataloader[k]) * epochs\n","      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n","\n","      for epoch in range(epochs):\n","        # Train the model for one epoch\n","        print('[%s] Beginning epoch...' % str(epoch))\n","\n","        epoch_loss, _ = train_epoch_plus_node(model, optimizer, train_fold_dataloader[k], device, node_embeddings_train, seg_mode=True)\n","\n","        # Validate on dev set\n","        results, _, _ = evaluate_plus_node(model, dev_fold_dataloader[k], device, node_embeddings_train, [(accuracy_score, 'accuracy')], seg_mode=True)\n","        print('[%s] Validation results:' % str(epoch))\n","        print_dict(results)\n","\n","        # Save accuracy\n","        acc = results['accuracy']\n","        if (bs, lr, epoch) in all_val_accs:\n","          all_val_accs[(bs, lr, epoch)] += acc\n","        else:\n","          all_val_accs[(bs, lr, epoch)] = acc\n","\n","      model.cpu()\n","      del model\n","      del optimizer\n","      del results\n","      del scheduler\n","      del total_steps\n","\n","      print('[%s] Finished epoch.' % str(epoch))\n","\n","for k in all_val_accs:\n","  all_val_accs[k] /= no_folds\n","\n","print('Top performing param combos:')\n","print(all_val_accs.most_common(5))\n","\n","save_fname = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models/%s_ConvEnt_xval_%s.pkl' % (model_name.replace('/','-'), '_'.join([str(lr) for lr in learning_rates])))\n","pickle.dump(all_val_accs, open(save_fname, 'wb'))"],"metadata":{"id":"c_ad-Zbp-RPf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734068528030,"user_tz":300,"elapsed":3397881,"user":{"displayName":"Jack Holland","userId":"09173278521114038765"}},"outputId":"fbb06032-e156-44c5-81a8-f1a255635b19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning grid search for ConvEnt over 1 parameter combination(s)!\n","\n","TRAINING MODEL: bs=32, lr=7.5e-06\n","Beginning fold 1/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5462962962962963,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[2] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.46296296296296297,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.46296296296296297,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.46296296296296297,\n","}\n","\n","\n","[7] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5370370370370371,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 2/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5581395348837209,\n","}\n","\n","\n","[1] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[4] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 3/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.6052631578947368,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.618421052631579,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.6447368421052632,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.3815789473684211,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.618421052631579,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.4605263157894737,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.3815789473684211,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5263157894736842,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 4/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.5057471264367817,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.4942528735632184,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5057471264367817,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5402298850574713,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5057471264367817,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5632183908045977,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.47126436781609193,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.5057471264367817,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 5/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.45,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.4875,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.4625,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.525,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 6/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.5454545454545454,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.4675324675324675,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.4805194805194805,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 7/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.4945054945054945,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5604395604395604,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5164835164835165,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-49a06d0514df>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.4835164835164835,\n","}\n","\n","\n","[7] Finished epoch.\n","Beginning fold 8/8...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[0] Validation results:\n","{\n","  accuracy: \n","    0.46938775510204084,\n","}\n","\n","\n","[1] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[1] Validation results:\n","{\n","  accuracy: \n","    0.6122448979591837,\n","}\n","\n","\n","[2] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:04s.\n","[2] Validation results:\n","{\n","  accuracy: \n","    0.6020408163265306,\n","}\n","\n","\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[3] Validation results:\n","{\n","  accuracy: \n","    0.5816326530612245,\n","}\n","\n","\n","[4] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[4] Validation results:\n","{\n","  accuracy: \n","    0.5102040816326531,\n","}\n","\n","\n","[5] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[5] Validation results:\n","{\n","  accuracy: \n","    0.5612244897959183,\n","}\n","\n","\n","[6] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[6] Validation results:\n","{\n","  accuracy: \n","    0.5306122448979592,\n","}\n","\n","\n","[7] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:03s.\n","[7] Validation results:\n","{\n","  accuracy: \n","    0.45918367346938777,\n","}\n","\n","\n","[7] Finished epoch.\n","Top performing param combos:\n","[((32, 7.5e-06, 2), 0.530557347100321), ((32, 7.5e-06, 1), 0.5297506015299962), ((32, 7.5e-06, 4), 0.5166809074720017), ((32, 7.5e-06, 0), 0.510735352707816), ((32, 7.5e-06, 5), 0.5048005234156226)]\n"]}]},{"cell_type":"markdown","source":["## Re-Train Best Model from Cross-Validation"],"metadata":{"id":"p-dg1cERvAZ4"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.eval import save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","from collections import Counter\n","\n","# Re-train the model with the best parameters from the grid search/cross-validation (with all folds)\n","xval_fnames = []\n","xval_fnames.append(save_fname.split('/')[-1])\n","\n","xval_results = Counter()\n","for fname in xval_fnames:\n","  xval_results += pickle.load(open(os.path.join(GOOGLE_DRIVE_PATH, 'saved_models/', fname), 'rb'))\n","\n","# batch_size, learning_rate, epochs = xval_results.most_common(1)[0][0]\n","batch_size, learning_rate, epochs = 32, 1e-05, 7\n","epochs += 1\n","\n","# Set up model\n","if 'mnli' not in mode:\n","  model = model_class(BertForSequenceClassification, tokenizer, model_name, cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","  # model = model_class.from_pretrained(model_name, cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","else:\n","  config = config_class.from_pretrained(model_name.replace('-mnli',''),\n","                                  num_labels=3,\n","                                  cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","  model = model_class.from_pretrained(model_name,\n","                                      config=config,\n","                                      cache_dir=os.path.join(GOOGLE_DRIVE_PATH, 'cache'))\n","  config.num_labels = 2\n","  model.num_labels = 2\n","  model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n","\n","model.cuda()\n","device = model.device\n","\n","train_sampler = RandomSampler(ConvEnt_train_tensor)\n","train_dataloader = DataLoader(ConvEnt_train_tensor, sampler=train_sampler, batch_size=batch_size)\n","\n","# Set up optimizer\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n","\n","for epoch in range(epochs):\n","  print('[%s] Beginning epoch...' % str(epoch))\n","  epoch_loss, _ = train_epoch_plus_node(model, optimizer, train_dataloader, device, node_embeddings_train, seg_mode=True if 'roberta' not in mode else False)\n","\n","print('[%s] Saving model checkpoint...' % str(epoch))\n","model_param_str = get_model_dir(model_name.replace('/','-'), 'ConvEnt', batch_size, learning_rate, epoch) + '_xval'\n","print(model_param_str)\n","output_dir = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', model_param_str)\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","model = model.module if hasattr(model, 'module') else model\n","\n","# Saving AugmentedModel does not work without custom save/from_pretrained function, rather just have to run all prior to load best model\n","# model.save_pretrained(output_dir)\n","tokenizer.save_vocabulary(output_dir)"],"metadata":{"id":"W1cUH9LqvLPN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734116777031,"user_tz":300,"elapsed":214772,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"3ef790ce-4c82-4837-f2db-d6d706392fbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[0] Beginning epoch...\n","[1] Beginning epoch...\n","[2] Beginning epoch...\n","[3] Beginning epoch...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-48-43102644430d>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_node_embeddings[item] = torch.tensor(input_node_embeddings[item]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["[4] Beginning epoch...\n","[5] Beginning epoch...\n","[6] Beginning epoch...\n","[7] Beginning epoch...\n","[7] Saving model checkpoint...\n","bert-large-uncased_ConvEnt_32_1e-05_7_xval\n"]},{"output_type":"execute_result","data":{"text/plain":["('drive/My Drive/CSE 595/FinalProject/saved_models/bert-large-uncased_ConvEnt_32_1e-05_7_xval/vocab.txt',)"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["## Test Models on Conversational Entailment"],"metadata":{"id":"sBTdoyfRvGRq"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.eval import evaluate, save_results, save_preds\n","from sklearn.metrics import accuracy_score\n","from www.utils import print_dict, get_model_dir\n","\n","best_model = eval_model_dir\n","\n","\n","best_model = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', best_model)\n","\n","# Saving AugmentedModel does not work without custome from_pretrained function, rather just have to run all prior to load best model\n","# print(best_model)\n","# Load the model\n","# model = model_class.from_pretrained(best_model)\n","# model.cuda()\n","# device = model.device\n","\n","# Select appropriate dataset\n","if 'cloze' in best_model:\n","  subtask = 'cloze'\n","elif 'order' in best_model:\n","  subtask = 'order'\n","\n","test_sampler = SequentialSampler(ConvEnt_test_tensor)\n","# test_dataloader = DataLoader(ConvEnt_test_tensor, sampler=test_sampler, batch_size=128)\n","test_dataloader = DataLoader(ConvEnt_test_tensor, sampler=test_sampler, batch_size=1)\n","test_dataset_name = '%s_%s' % ('ConvEnt', 'test')\n","test_ids = [str(ex['example_id']) for ex in ConvEnt_test]\n","\n","print('Testing model: %s.' % best_model.split('/')[-1])\n","\n","results, preds, labels = evaluate_plus_node(model, test_dataloader, device, node_embeddings_test, [(accuracy_score, 'accuracy')], seg_mode=True if 'roberta' not in mode else False)\n","save_results(results, best_model, test_dataset_name)\n","save_preds(test_ids, labels, preds, best_model, test_dataset_name)\n","\n","print('Results:')\n","print_dict(results)"],"metadata":{"id":"UBjA5BdmvKTo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734118708758,"user_tz":300,"elapsed":6534,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"136e9f52-bc68-4e0d-ceeb-3f093310956c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=\"cpu\")\n"]},{"output_type":"stream","name":"stdout","text":["Testing model: bert-large-uncased_ConvEnt_32_1e-05_7_xval.\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:01s.\n","Results:\n","{\n","  accuracy: \n","    0.6162790697674418,\n","}\n","\n","\n"]}]},{"cell_type":"markdown","source":["## Coherence Checks on Conversational Entailment"],"metadata":{"id":"myuG9UGkvLk4"}},{"cell_type":"markdown","source":["### Load and Featurize Span Data"],"metadata":{"id":"gMZsVsUdvP0P"}},{"cell_type":"code","source":["from www.dataset.prepro import get_ConvEnt_spans\n","import pickle\n","seq_length = 128\n","\n","merged_file = os.path.join(GOOGLE_DRIVE_PATH, 'data/ConvEnt_test_annotation_merged2.json')\n","ConvEnt_test = json.load(open(merged_file))\n","\n","cache_embeddings_test_merged = os.path.join(GOOGLE_DRIVE_PATH,'data/ConvEnt_dev_node_embeddings_test_merged.pkl')\n","\n","ConvEnt_test, node_embeddings_test_merged = add_bert_features_ConvEnt_plus_node(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True, node_embeddings_from_storage=True, node_embeddings_path=cache_embeddings_test_merged)\n","\n","# Cache node_embeddings_test\n","if not os.path.exists(cache_embeddings_test_merged):\n","  print('Caching node embeddings...')\n","  # print(node_embeddings_test_merged)\n","  pickle.dump(node_embeddings_test_merged, open(cache_embeddings_test_merged, 'wb'))\n","\n","\n","if debug:\n","  ConvEnt_test = ConvEnt_test[:10]\n","\n","# Some of the annotated examples are no longer in the test set :(\n","# ConvEnt_test = [ex for ex in ConvEnt_test if ex['id'] in test_ids]\n","\n","# Make span versions of the datasets\n","ConvEnt_test_spans = get_ConvEnt_spans(ConvEnt_test)\n","\n","# Add BERT features\n","ConvEnt_test_tensor = get_tensor_dataset_plus_node(ConvEnt_test, label_key='label', add_segment_ids=True)\n","ConvEnt_test_spans_tensor = get_tensor_dataset_plus_node(ConvEnt_test_spans, label_key='label', add_segment_ids=True)"],"metadata":{"id":"hrNpFFtfvTYB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734116784750,"user_tz":300,"elapsed":1603,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"e41b6d55-bd5d-4c29-d7c6-379cff1de0fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading node embeddings from storage...\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]}]},{"cell_type":"markdown","source":["### Load the Trained Model"],"metadata":{"id":"BLspbT5YvS9S"}},{"cell_type":"code","source":["probe_model = eval_model_dir\n","probe_model = os.path.join(GOOGLE_DRIVE_PATH, 'saved_models', probe_model)\n","\n","# Saving AugmentedModel does not work without custom save/from_pretrained function, rather just have to run all prior to load best model\n","# Load the model\n","# model = model_class.from_pretrained(probe_model)\n","# if torch.cuda.is_available():\n","#   model.cuda()\n","# device = model.device"],"metadata":{"id":"Vonoa8g8vWoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from www.model.eval import load_preds\n","from www.utils import print_dict\n","\n","preds_base = {}\n","preds_base['test'] = load_preds(os.path.join(probe_model, 'preds_ConvEnt_test.tsv'))\n","print(preds_base['test'].keys())"],"metadata":{"id":"Q3tgZQTO167a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734116784840,"user_tz":300,"elapsed":2,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"c47e1899-66c7-4a9d-b85f-0e91631ace3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '272', '273', '274', '275', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838'])\n"]}]},{"cell_type":"markdown","source":["### Check a Model"],"metadata":{"id":"h8_tNAlcvW3z"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from www.model.eval import save_results, save_preds, list_comparison\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n","import numpy as np\n","from www.utils import print_dict\n","\n","def is_polarized(smax, thres):\n","  return (abs(smax[0] - smax[1]) >= thres)\n","\n","print('Testing model: %s.' % probe_model)\n","\n","all_results = {}\n","p = 'test'\n","\n","p_dataset = ConvEnt_test_spans\n","p_tensor_dataset = ConvEnt_test_spans_tensor\n","p_sampler = SequentialSampler(p_tensor_dataset)\n","p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=512)\n","p_dataset_name = '%s_spans_%s' % ('ConvEnt', p)\n","p_dataset_name_co = '%s_consistent_%s' % ('ConvEnt', p)\n","p_dataset_name_bp = '%s_breakpoints_%s' % ('ConvEnt', p)\n","p_dataset_name_ev = '%s_evidence_%s' % ('ConvEnt', p)\n","p_dataset_name_coh = '%s_coherent_%s' % ('ConvEnt', p)\n","p_ids = [str(ex['example_id']) for ex in ConvEnt_test_spans]\n","p_labels = [ex['label'] for ex in ConvEnt_test_spans]\n","\n","# Get span preds and save metrics\n","results, preds, labels = evaluate_plus_node(model, p_dataloader, device, node_embeddings_test, metrics, seg_mode=True)\n","save_results(results, probe_model, p_dataset_name)\n","save_preds(p_ids, labels, preds, probe_model, p_dataset_name)\n","\n","# Convert substory preds into breakpoint preds for each example\n","ids_base = [str(ex['example_id']) for ex in ConvEnt_test]\n","\n","id_to_pred = {k: v for k,v in zip(p_ids, preds)}\n","id_to_label = {k: v for k,v in zip(p_ids, p_labels)}\n","\n","preds_entailment = []\n","labels_entailment = []\n","preds_consistent = []\n","preds_breakpoint = []\n","labels_breakpoint = []\n","preds_evidence = []\n","labels_evidence = []\n","span_accuracies = []\n","span_accuracies_strict = []\n","preds_coherent = []\n","\n","for i, exid in enumerate(ids_base):\n","  ex = ConvEnt_test[i]\n","  ex['length'] = len(ex['turns'])\n","\n","  label_entailment = preds_base[p][exid]['label']\n","  pred_entailment = preds_base[p][exid]['pred']\n","  labels_entailment.append(label_entailment)\n","  preds_entailment.append(pred_entailment)\n","\n","  # Get ground truth breakpoint and evidence\n","  label_breakpoint = ex['conflict_pair'][1] if ex['conflict_pair'] is not None and len(ex['conflict_pair']) > 0 else 0\n","  labels_breakpoint.append(label_breakpoint)\n","  if label_breakpoint > 0:\n","    label_ev = ex['conflict_pair'][0]\n","  else:\n","    label_ev = -1\n","  labels_evidence.append(label_ev)\n","\n","  # Check consistency - any span that entails the hypothesis' superspans should also entail\n","  pred_consistent = True\n","  span_accuracy = 0.0\n","  span_accuracy_strict = 0.0\n","  pred_coherent = True\n","\n","  no_spans = 0\n","  for sp1 in range(ex['length']):\n","    if not pred_consistent:\n","      break\n","\n","    for sp2 in range(sp1, ex['length']):\n","      if not pred_consistent:\n","        break\n","\n","      span_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n","      span_label = id_to_label[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n","\n","      if span_pred == span_label:\n","        span_accuracy += 1.0\n","        if label_entailment == pred_entailment:\n","            span_accuracy_strict += 1.0\n","      else:\n","        pred_coherent = False\n","      no_spans += 1\n","      # print('%s:%s\\t%s\\t(%s, %s)' % (str(sp1), str(sp2), str(span_pred), str(span_prob[0]), str(span_prob[1])))\n","\n","      if span_pred == 1:\n","        if pred_entailment == 1:\n","          for sp3 in range(sp1+1):\n","            if not pred_consistent:\n","              break\n","\n","            for sp4 in range(sp2, ex['length']):\n","              if not pred_consistent:\n","                break\n","\n","              sspan_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp3), str(sp4))]\n","\n","              if sspan_pred == 0:\n","                pred_consistent = False\n","                break\n","        elif pred_entailment == 0:\n","          pred_consistent = False\n","\n","  preds_consistent.append(1 if pred_consistent else 0)\n","  span_accuracies.append(span_accuracy / no_spans)\n","  span_accuracies_strict.append(span_accuracy_strict / no_spans)\n","  preds_coherent.append(1 if pred_coherent else 0)\n","\n","  # Check pred. breakpoint (verifiability) - will be first sentence where the model prediction becomes polarized, i.e., confidence > threshold\n","  pred_breakpoint = 0 # For now, 0 means -1, i.e., stories are entirely plausible - this shouldn't happen but it will (inconsistent?)\n","  for ss in range(1, ex['length']):\n","    if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n","      pred_breakpoint = ss\n","      break\n","  preds_breakpoint.append(pred_breakpoint)\n","\n","  # Check pred. evidence (verifiability)\n","  if pred_breakpoint > 0:\n","    pred_evidence = -1\n","    for ss in range(0, pred_breakpoint+1):\n","      if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n","        pred_evidence = ss\n","  else:\n","    pred_evidence = -1 # This should never happen - it would be inconsistent if it did\n","  preds_evidence.append(pred_evidence)\n","\n","# Calculate tiered accuracy for model\n","acc = 0\n","acc_con = 0\n","acc_con_vbp = 0\n","acc_con_vbp_vev = 0\n","no_ex = len(ids_base)\n","for p_plaus, l_plaus, con, p_bp, l_bp, p_ev, l_ev in zip(preds_entailment, labels_entailment, preds_consistent, preds_breakpoint, labels_breakpoint, preds_evidence, labels_evidence):\n","  # Accuracy\n","  if p_plaus == l_plaus:\n","    acc += 1\n","\n","    # Consistency\n","    if con == 1:\n","      acc_con += 1\n","\n","      # Verifiability (breakpoint)\n","      if p_bp == l_bp:\n","        acc_con_vbp += 1\n","\n","        # Verifiability (evidence)\n","        if p_ev == l_ev:\n","          acc_con_vbp_vev += 1\n","\n","acc /= no_ex\n","acc_con /= no_ex\n","acc_con_vbp /= no_ex\n","acc_con_vbp_vev /= no_ex\n","\n","# all_results['acc'] = acc\n","# all_results['acc_con'] = acc_con\n","# all_results['acc_con_vbp'] = acc_con_vbp\n","# all_results['acc_con_vbp_vev'] = acc_con_vbp_vev\n","# all_results['span_accuracy'] = np.mean(span_accuracies)\n","\n","all_results['lenient_coherence'] = np.mean(span_accuracies_strict)\n","all_results['strict_coherence'] = np.mean(preds_coherent)\n","\n","best_preds_entailment = preds_entailment\n","best_preds_consistent = preds_consistent\n","best_preds_breakpoint = preds_breakpoint\n","best_preds_evidence = preds_evidence\n","best_preds_coherent = preds_coherent\n","\n","print('\\nPARTITION: %s' % p)\n","print_dict(all_results)\n","\n","# Save preds for breakpoint and evidence\n","save_preds(ids_base, np.array(labels_breakpoint), best_preds_breakpoint, probe_model, p_dataset_name_bp)\n","save_preds(ids_base, np.array(labels_evidence), best_preds_evidence, probe_model, p_dataset_name_ev)\n","save_preds(ids_base, np.array([1 for p in best_preds_coherent]), best_preds_coherent, probe_model, p_dataset_name_coh)\n","\n","p_dataset_name_agg = '%s_tiers_agg_nostates_lenient_%s' % ('ConvEnt', p)\n","save_results(all_results, probe_model, p_dataset_name_agg)"],"metadata":{"id":"zI-6yYn5vZWW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734118723018,"user_tz":300,"elapsed":8233,"user":{"displayName":"Jack Holland","userId":"01508320804054895894"}},"outputId":"4234b542-7e7c-4ae8-fcad-9657939a4bca"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing model: drive/My Drive/CSE 595/FinalProject/saved_models/bert-large-uncased_ConvEnt_32_1e-05_7_xval.\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n","\t\tComputing metrics...\n","\tFinished evaluation in 0:00:05s.\n","\n","PARTITION: test\n","{\n","  lenient_coherence: \n","    0.4137883624203733,\n","  strict_coherence: \n","    0.3430232558139535,\n","}\n","\n","\n"]}]}]}